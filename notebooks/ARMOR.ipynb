{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Unable to write current GraphLab Create license to /home/alvas/.graphlab/config. Ensure that this user account has write permission to /home/alvas/.graphlab/config to save the license for offline use.\n",
      "[INFO] This non-commercial license of GraphLab Create is assigned to liling@coli.uni-saarland.de and will expire on October 11, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-3533 - Server binary: /usr/local/lib/python2.7/dist-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1454465611.log\n",
      "[INFO] GraphLab Server Version: 1.8.1\n",
      "[WARNING] Unable to create session in specified location: '/home/alvas/.graphlab/artifacts'. Using: '/var/tmp/graphlab-alvas/3533/tmp_session_01badd90-ad5f-4745-9dba-d0202b9fcc52'\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from HAMMER import train_xgboost, xgboost_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sts_train = gl.SFrame('../sts2016_train.stasis/')\n",
    "sts_test = gl.SFrame('../sts2016_test.stasis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sts_train = sts_train.dropna(columns=['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import numpy\n",
    "\n",
    "\n",
    "train = sts_train\n",
    "\n",
    "feat = [i for i in train.column_names() if i not in ['Dataset', 'Domain', 'Score', 'Sent1', 'Sent2']]\n",
    "\n",
    "train_valid_shuffled = graphlab.toolkits.cross_validation.shuffle(train, random_seed=1)\n",
    "\n",
    "def kth_start_end(n,k,kth):\n",
    "    start = (n*kth)/k\n",
    "    end = (n*(kth+1))/k-1\n",
    "    return start, end+1\n",
    "\n",
    "def get_kth_train(data, kth_start, kth_end, n):\n",
    "    part1 = data[0:kth_start]\n",
    "    part2 = data[kth_end+1:n]\n",
    "    return part1.append(part2)\n",
    "\n",
    "n = len(train)\n",
    "accuracy = []\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = [i for i in train.column_names() if i not in ['Dataset', 'Domain', 'Score', 'Sent1', 'Sent2'] + \\\n",
    "        ['DLS_compose_ppmi', 'DLS_compose_cbow', 'prop_harmonic', 'glove_cosine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SIZE_QUANTITY)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12237\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 1.195950     | 4.30276            | 2.12137       |\n",
      "PROGRESS: | 2         | 1.262646     | 3.94695            | 1.62882       |\n",
      "PROGRESS: | 3         | 1.326609     | 3.72568            | 1.31028       |\n",
      "PROGRESS: | 4         | 1.391749     | 3.68575            | 1.11025       |\n",
      "PROGRESS: | 5         | 1.457453     | 3.59852            | 0.98721       |\n",
      "PROGRESS: | 6         | 1.528658     | 3.49214            | 0.911706      |\n",
      "PROGRESS: | 7         | 1.594416     | 3.50628            | 0.866854      |\n",
      "PROGRESS: | 8         | 1.659107     | 3.57003            | 0.835718      |\n",
      "PROGRESS: | 9         | 1.725706     | 3.3707             | 0.814964      |\n",
      "PROGRESS: | 10        | 1.793324     | 3.38723            | 0.798882      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.79600640778291609, 3.6072769373312225e-298)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.185914     | 4.1646             | 2.12172       |\n",
      "PROGRESS: | 2         | 0.254056     | 3.87986            | 1.6302        |\n",
      "PROGRESS: | 3         | 0.319827     | 3.76545            | 1.3108        |\n",
      "PROGRESS: | 4         | 0.386639     | 3.52938            | 1.11065       |\n",
      "PROGRESS: | 5         | 0.454501     | 3.34138            | 0.988828      |\n",
      "PROGRESS: | 6         | 0.521447     | 3.34913            | 0.915225      |\n",
      "PROGRESS: | 7         | 0.586850     | 3.28457            | 0.867169      |\n",
      "PROGRESS: | 8         | 0.652413     | 3.344              | 0.837691      |\n",
      "PROGRESS: | 9         | 0.720270     | 3.27007            | 0.815566      |\n",
      "PROGRESS: | 10        | 0.787716     | 3.21599            | 0.799695      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.80840727515982802, 7.3639715795899433e-315)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.285817     | 4.0985             | 2.11909       |\n",
      "PROGRESS: | 2         | 0.374336     | 3.90534            | 1.62699       |\n",
      "PROGRESS: | 3         | 0.468724     | 3.77446            | 1.30923       |\n",
      "PROGRESS: | 4         | 0.552250     | 3.57529            | 1.11158       |\n",
      "PROGRESS: | 5         | 0.658697     | 3.52793            | 0.988643      |\n",
      "PROGRESS: | 6         | 0.764267     | 3.554              | 0.914469      |\n",
      "PROGRESS: | 7         | 0.840792     | 3.41804            | 0.867671      |\n",
      "PROGRESS: | 8         | 0.908142     | 3.44031            | 0.837415      |\n",
      "PROGRESS: | 9         | 0.978787     | 3.45569            | 0.817892      |\n",
      "PROGRESS: | 10        | 1.044077     | 3.48399            | 0.803501      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.79740944123432644, 3.4143664089118228e-300)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12237\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.279387     | 4.16332            | 2.12586       |\n",
      "PROGRESS: | 2         | 0.348409     | 3.94884            | 1.63289       |\n",
      "PROGRESS: | 3         | 0.414777     | 3.762              | 1.31414       |\n",
      "PROGRESS: | 4         | 0.482776     | 3.59993            | 1.11439       |\n",
      "PROGRESS: | 5         | 0.552686     | 3.57735            | 0.992775      |\n",
      "PROGRESS: | 6         | 0.640720     | 3.40151            | 0.916758      |\n",
      "PROGRESS: | 7         | 0.709809     | 3.36011            | 0.871699      |\n",
      "PROGRESS: | 8         | 0.779806     | 3.43679            | 0.840452      |\n",
      "PROGRESS: | 9         | 0.851264     | 3.52301            | 0.816837      |\n",
      "PROGRESS: | 10        | 0.928616     | 3.54247            | 0.801853      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.82750961724809291, 0.0)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(LST)', 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.243861     | 4.17374            | 2.12113       |\n",
      "PROGRESS: | 2         | 0.340318     | 3.89235            | 1.62766       |\n",
      "PROGRESS: | 3         | 0.433832     | 3.79673            | 1.3087        |\n",
      "PROGRESS: | 4         | 0.502608     | 3.61114            | 1.10814       |\n",
      "PROGRESS: | 5         | 0.570847     | 3.51913            | 0.985214      |\n",
      "PROGRESS: | 6         | 0.641964     | 3.54593            | 0.912666      |\n",
      "PROGRESS: | 7         | 0.711724     | 3.49228            | 0.865142      |\n",
      "PROGRESS: | 8         | 0.782201     | 3.39322            | 0.834763      |\n",
      "PROGRESS: | 9         | 0.853826     | 3.48367            | 0.814273      |\n",
      "PROGRESS: | 10        | 0.919641     | 3.53482            | 0.798854      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.80425716256845992, 3.2448354104068082e-309)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.198691     | 4.32526            | 2.11641       |\n",
      "PROGRESS: | 2         | 0.266672     | 4.17597            | 1.626         |\n",
      "PROGRESS: | 3         | 0.333214     | 4.07524            | 1.30801       |\n",
      "PROGRESS: | 4         | 0.399683     | 3.77366            | 1.10842       |\n",
      "PROGRESS: | 5         | 0.468746     | 3.674              | 0.985745      |\n",
      "PROGRESS: | 6         | 0.537004     | 3.38386            | 0.911934      |\n",
      "PROGRESS: | 7         | 0.653002     | 3.36778            | 0.866096      |\n",
      "PROGRESS: | 8         | 0.771876     | 3.41947            | 0.835884      |\n",
      "PROGRESS: | 9         | 0.869399     | 3.45798            | 0.814644      |\n",
      "PROGRESS: | 10        | 0.937904     | 3.5176             | 0.79818       |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.80813790179081013, 1.7283727003219313e-314)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12237\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.191887     | 4.1963             | 2.12084       |\n",
      "PROGRESS: | 2         | 0.260591     | 3.93638            | 1.62934       |\n",
      "PROGRESS: | 3         | 0.326202     | 3.90507            | 1.30997       |\n",
      "PROGRESS: | 4         | 0.392864     | 3.91273            | 1.11065       |\n",
      "PROGRESS: | 5         | 0.457075     | 3.90801            | 0.987379      |\n",
      "PROGRESS: | 6         | 0.523171     | 3.87978            | 0.912042      |\n",
      "PROGRESS: | 7         | 0.592546     | 3.52666            | 0.865571      |\n",
      "PROGRESS: | 8         | 0.658784     | 3.53962            | 0.834768      |\n",
      "PROGRESS: | 9         | 0.726072     | 3.56896            | 0.81143       |\n",
      "PROGRESS: | 10        | 0.793267     | 3.56356            | 0.793686      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.80395771623179768, 1.3773703337686225e-308)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.191874     | 4.37079            | 2.11632       |\n",
      "PROGRESS: | 2         | 0.261126     | 3.97568            | 1.62591       |\n",
      "PROGRESS: | 3         | 0.323893     | 3.63188            | 1.30902       |\n",
      "PROGRESS: | 4         | 0.393386     | 3.65442            | 1.11123       |\n",
      "PROGRESS: | 5         | 0.455545     | 3.48703            | 0.990188      |\n",
      "PROGRESS: | 6         | 0.521348     | 3.48201            | 0.915922      |\n",
      "PROGRESS: | 7         | 0.588312     | 3.33653            | 0.869434      |\n",
      "PROGRESS: | 8         | 0.654527     | 3.39333            | 0.838709      |\n",
      "PROGRESS: | 9         | 0.725820     | 3.42488            | 0.817529      |\n",
      "PROGRESS: | 10        | 0.798290     | 3.48306            | 0.803978      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.81747424375765798, 0.0)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12236\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.187611     | 4.25071            | 2.11927       |\n",
      "PROGRESS: | 2         | 0.266885     | 4.04999            | 1.62864       |\n",
      "PROGRESS: | 3         | 0.334938     | 3.93502            | 1.30774       |\n",
      "PROGRESS: | 4         | 0.401632     | 3.80336            | 1.10943       |\n",
      "PROGRESS: | 5         | 0.468235     | 3.79175            | 0.988425      |\n",
      "PROGRESS: | 6         | 0.547635     | 3.73082            | 0.914021      |\n",
      "PROGRESS: | 7         | 0.620710     | 3.64741            | 0.868005      |\n",
      "PROGRESS: | 8         | 0.687520     | 3.57862            | 0.83811       |\n",
      "PROGRESS: | 9         | 0.756677     | 3.41294            | 0.817672      |\n",
      "PROGRESS: | 10        | 0.824492     | 3.19032            | 0.800884      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.8001150275161405, 1.0248766046500093e-303)\n",
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 12237\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.277767     | 4.22332            | 2.12066       |\n",
      "PROGRESS: | 2         | 0.353088     | 3.85984            | 1.6302        |\n",
      "PROGRESS: | 3         | 0.420859     | 3.82914            | 1.31234       |\n",
      "PROGRESS: | 4         | 0.491214     | 3.64378            | 1.11154       |\n",
      "PROGRESS: | 5         | 0.569710     | 3.69175            | 0.990302      |\n",
      "PROGRESS: | 6         | 0.671885     | 3.59462            | 0.91557       |\n",
      "PROGRESS: | 7         | 0.785208     | 3.51333            | 0.868979      |\n",
      "PROGRESS: | 8         | 0.851789     | 3.52623            | 0.839164      |\n",
      "PROGRESS: | 9         | 0.925904     | 3.6122             | 0.818939      |\n",
      "PROGRESS: | 10        | 0.992015     | 3.658              | 0.803941      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "(0.8172053312777765, 0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(k):\n",
    "    start, end = kth_start_end(n,k,i)\n",
    "    valid_kth = train_valid_shuffled[start:end]\n",
    "    train_kth = get_kth_train(train_valid_shuffled, start, end, n)\n",
    "    m = graphlab.boosted_trees_regression.create(train_kth, target='Score', features=feat, validation_set=None)\n",
    "    print sp.stats.pearsonr(np.array(m.predict(valid_kth)), np.array(valid_kth['Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2015 = train.filter_by('STS2015-gold', 'Dataset')\n",
    "traintill2014 = train.filter_by(['STS2012-gold', 'STS2013-gold', 'STS2014-gold'], 'Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', 'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SIZE_QUANTITY)', 'NE-Me(SPEED_QUANTITY)', 'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', 'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', 'NE-Oe(PROJECT)', 'NE-Oe(SIZE_QUANTITY)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', 'NE-Oe(TEMPERATURE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8363\n",
      "PROGRESS: Number of features          : 122\n",
      "PROGRESS: Number of unpacked features : 122\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.132799     | 4.32413            | 2.16439       |\n",
      "PROGRESS: | 2         | 0.180161     | 4.27158            | 1.66565       |\n",
      "PROGRESS: | 3         | 0.227444     | 4.03262            | 1.33761       |\n",
      "PROGRESS: | 4         | 0.275449     | 3.80303            | 1.13102       |\n",
      "PROGRESS: | 5         | 0.329252     | 3.45788            | 1.00191       |\n",
      "PROGRESS: | 6         | 0.377523     | 3.26605            | 0.917987      |\n",
      "PROGRESS: | 7         | 0.421227     | 3.2776             | 0.865446      |\n",
      "PROGRESS: | 8         | 0.466966     | 3.26547            | 0.836075      |\n",
      "PROGRESS: | 9         | 0.517211     | 3.09813            | 0.812102      |\n",
      "PROGRESS: | 10        | 0.567130     | 3.09856            | 0.7958        |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "m1 = graphlab.boosted_trees_regression.create(traintill2014, target='Score', features=feat, validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76747146564404811, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.stats.pearsonr(np.array(m1.predict(test2015)), np.array(test2015['Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: WARNING: Detected extremely low variance for feature(s) 'NE-Me(SIZE_QUANTITY)', 'NE-Oe(SIZE_QUANTITY)' because all entries are nearly the same.\n",
      "Proceeding with model training using all features. If the model does not provide results of adequate quality, exclude the above mentioned feature(s) from the input dataset.\n",
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8363\n",
      "PROGRESS: Number of features          : 104\n",
      "PROGRESS: Number of unpacked features : 104\n",
      "PROGRESS: Number of coefficients    : 105\n",
      "PROGRESS: Starting Newton Method\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 2        | 0.056409     | 5.538508           | 0.952426      |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.80060123152467633, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = [i for i in train.column_names() if i not in ['Dataset', 'Domain', 'Score', 'Sent1', 'Sent2'] + \\\n",
    "        ['DLS_compose_ppmi', 'DLS_cbow_ppmi', 'prop_harmonic', 'glove_cosine']]\n",
    "\n",
    "bad_features = ['SP-Oc(UCP)', 'NE-Me(ANGLE_QUANTITY)', 'NE-Me(LANGUAGE)', 'NE-Me(MEASURE)', \n",
    "                'NE-Me(METHOD)', 'NE-Me(NUM)', 'NE-Me(PROJECT)', 'NE-Me(SPEED_QUANTITY)', \n",
    "                'NE-Me(SYSTEM)', 'NE-Me(TEMPERATURE_QUANTITY)', 'NE-Oe(ANGLE_QUANTITY)', \n",
    "                'NE-Oe(LANGUAGE)', 'NE-Oe(MEASURE)', 'NE-Oe(METHOD)', 'NE-Oe(NUM)', \n",
    "                'NE-Oe(PROJECT)', 'NE-Oe(SPEED_QUANTITY)', 'NE-Oe(SYSTEM)', \n",
    "                'NE-Oe(TEMPERATURE_QUANTITY)']\n",
    "\n",
    "feat2 = [_f for _f in feat if _f not in bad_features]\n",
    "\n",
    "m2 = graphlab.linear_regression.create(traintill2014, target='Score', features=feat2, validation_set=None)\n",
    "sp.stats.pearsonr(np.array(m2.predict(test2015)), np.array(test2015['Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8363\n",
      "PROGRESS: Number of features          : 102\n",
      "PROGRESS: Number of unpacked features : 102\n",
      "PROGRESS: Number of coefficients    : 103\n",
      "PROGRESS: Starting Newton Method\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 2        | 0.056409     | 5.538508           | 0.952426      |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.80060123152467633, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_features = ['NE-Me(SIZE_QUANTITY)', 'NE-Oe(SIZE_QUANTITY)']\n",
    "\n",
    "feat3 = [_f for _f in feat2 if _f not in bad_features]\n",
    "m3 = graphlab.linear_regression.create(traintill2014, target='Score', features=feat3, validation_set=None)\n",
    "sp.stats.pearsonr(np.array(m3.predict(test2015)), np.array(test2015['Score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------+----------------+-----------------+\n",
      "|           name           | index |     value      |      stderr     |\n",
      "+--------------------------+-------+----------------+-----------------+\n",
      "|   NGRAM-cosChar4ngrams   |  None | 6.64541329135  |  1.38523937316  |\n",
      "|   NGRAM-jacTok4ngrams    |  None | 6.06758408044  |  3.24606370942  |\n",
      "|   NGRAM-jacChar3ngrams   |  None | 5.05746458861  |  1.06740806321  |\n",
      "| NE-Me(DISTANCE_QUANTITY) |  None | 4.39902015316  |   1.8684383491  |\n",
      "|   NGRAM-cosTok3ngrams    |  None | 3.51442526391  |  1.37728302729  |\n",
      "|      METEOR-synonym      |  None | 3.15308000549  |  0.372721771383 |\n",
      "|     DLS_compose_cbow     |  None | 2.89948587079  |  0.106539261168 |\n",
      "|    METEOR-paraphrase     |  None | 2.65927743384  |  0.161514123733 |\n",
      "|        METEOR-all        |  None |  2.6173040982  |  0.248316703592 |\n",
      "|   NGRAM-jacTok2ngrams    |  None | 1.85492226175  |  1.01206294389  |\n",
      "|       NE-Me(TIME)        |  None |  1.4508643107  |  0.683320757727 |\n",
      "|       NE-Me(MONEY)       |  None | 1.32852843844  |  0.436395917468 |\n",
      "|      NE-Me(PERCENT)      |  None | 1.29001593175  |  0.384262458354 |\n",
      "|    NGRAM-jacCognates     |  None | 1.08239455214  |  0.124200656165 |\n",
      "|          REVAL           |  None | 0.884119111095 | 0.0801082076083 |\n",
      "|        SP-Oc(LST)        |  None | 0.766597152405 |  0.964372639459 |\n",
      "|   NGRAM-jacChar2ngrams   |  None | 0.730139144208 |  0.349249796152 |\n",
      "|       SP-Oc(INTJ)        |  None | 0.69989336419  |  0.963239764564 |\n",
      "|      SP-iobNISTi-2       |  None | 0.397299490846 |  226.844855774  |\n",
      "|       NE-Oe(MISC)        |  None | 0.388911886152 |  0.197457684287 |\n",
      "+--------------------------+-------+----------------+-----------------+\n",
      "[103 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m3.coefficients.sort('value', ascending=False).print_rows(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8363\n",
      "PROGRESS: Number of features          : 102\n",
      "PROGRESS: Number of unpacked features : 102\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.111682     | 4.01133            | 2.15363       |\n",
      "PROGRESS: | 2         | 0.169839     | 3.87901            | 1.64403       |\n",
      "PROGRESS: | 3         | 0.209472     | 3.66795            | 1.31346       |\n",
      "PROGRESS: | 4         | 0.252653     | 3.62628            | 1.1061        |\n",
      "PROGRESS: | 5         | 0.297478     | 3.63752            | 0.979447      |\n",
      "PROGRESS: | 6         | 0.344821     | 3.64008            | 0.896498      |\n",
      "PROGRESS: | 7         | 0.387687     | 3.67289            | 0.844464      |\n",
      "PROGRESS: | 8         | 0.433286     | 3.69975            | 0.812548      |\n",
      "PROGRESS: | 9         | 0.473889     | 3.45915            | 0.784092      |\n",
      "PROGRESS: | 10        | 0.514236     | 3.43229            | 0.766283      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8363\n",
      "PROGRESS: Number of features          : 102\n",
      "PROGRESS: Number of unpacked features : 102\n",
      "PROGRESS: Number of coefficients    : 103\n",
      "PROGRESS: Starting Newton Method\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 2        | 0.056691     | 5.538508           | 0.952426      |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "(0.78413241874194251, 0.0)\n",
      "(0.80060123152467633, 0.0)\n"
     ]
    }
   ],
   "source": [
    "m1_old = graphlab.boosted_trees_regression.create(traintill2014, target='Score', features=feat3, validation_set=None)\n",
    "m3_old = graphlab.linear_regression.create(traintill2014, target='Score', features=feat3, validation_set=None)\n",
    "print sp.stats.pearsonr(np.array(m1_old.predict(test2015)), np.array(test2015['Score']))\n",
    "print sp.stats.pearsonr(np.array(m3_old.predict(test2015)), np.array(test2015['Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmse:0.913444\teval-rmse:0.913698\n",
      "[1]\ttrain-rmse:0.896899\teval-rmse:0.897936\n",
      "[2]\ttrain-rmse:0.880650\teval-rmse:0.882264\n",
      "[3]\ttrain-rmse:0.864816\teval-rmse:0.867017\n",
      "[4]\ttrain-rmse:0.849292\teval-rmse:0.852042\n",
      "[5]\ttrain-rmse:0.834439\teval-rmse:0.837768\n",
      "[6]\ttrain-rmse:0.819591\teval-rmse:0.823518\n",
      "[7]\ttrain-rmse:0.805024\teval-rmse:0.809559\n",
      "[8]\ttrain-rmse:0.790751\teval-rmse:0.795930\n",
      "[9]\ttrain-rmse:0.776786\teval-rmse:0.782566\n",
      "[10]\ttrain-rmse:0.763067\teval-rmse:0.769620\n",
      "[11]\ttrain-rmse:0.749878\teval-rmse:0.757167\n",
      "[12]\ttrain-rmse:0.737139\teval-rmse:0.745018\n",
      "[13]\ttrain-rmse:0.724673\teval-rmse:0.733342\n",
      "[14]\ttrain-rmse:0.712128\teval-rmse:0.721542\n",
      "[15]\ttrain-rmse:0.699797\teval-rmse:0.709895\n",
      "[16]\ttrain-rmse:0.687786\teval-rmse:0.698649\n",
      "[17]\ttrain-rmse:0.676195\teval-rmse:0.687890\n",
      "[18]\ttrain-rmse:0.664571\teval-rmse:0.677157\n",
      "[19]\ttrain-rmse:0.653187\teval-rmse:0.666721\n",
      "[20]\ttrain-rmse:0.642350\teval-rmse:0.656604\n",
      "[21]\ttrain-rmse:0.631481\teval-rmse:0.646456\n",
      "[22]\ttrain-rmse:0.621117\teval-rmse:0.637005\n",
      "[23]\ttrain-rmse:0.610630\teval-rmse:0.627438\n",
      "[24]\ttrain-rmse:0.600432\teval-rmse:0.618105\n",
      "[25]\ttrain-rmse:0.590766\teval-rmse:0.609276\n",
      "[26]\ttrain-rmse:0.581160\teval-rmse:0.600405\n",
      "[27]\ttrain-rmse:0.571560\teval-rmse:0.591694\n",
      "[28]\ttrain-rmse:0.562466\teval-rmse:0.583373\n",
      "[29]\ttrain-rmse:0.553370\teval-rmse:0.574996\n",
      "[30]\ttrain-rmse:0.544310\teval-rmse:0.566873\n",
      "[31]\ttrain-rmse:0.535770\teval-rmse:0.559164\n",
      "[32]\ttrain-rmse:0.527293\teval-rmse:0.551499\n",
      "[33]\ttrain-rmse:0.518822\teval-rmse:0.543972\n",
      "[34]\ttrain-rmse:0.510762\teval-rmse:0.536828\n",
      "[35]\ttrain-rmse:0.502738\teval-rmse:0.529560\n",
      "[36]\ttrain-rmse:0.495093\teval-rmse:0.522804\n",
      "[37]\ttrain-rmse:0.487414\teval-rmse:0.516051\n",
      "[38]\ttrain-rmse:0.479863\teval-rmse:0.509363\n",
      "[39]\ttrain-rmse:0.472463\teval-rmse:0.502684\n",
      "[40]\ttrain-rmse:0.465223\teval-rmse:0.496358\n",
      "[41]\ttrain-rmse:0.458129\teval-rmse:0.490110\n",
      "[42]\ttrain-rmse:0.451339\teval-rmse:0.484150\n",
      "[43]\ttrain-rmse:0.444687\teval-rmse:0.478400\n",
      "[44]\ttrain-rmse:0.438146\teval-rmse:0.472861\n",
      "[45]\ttrain-rmse:0.431707\teval-rmse:0.467423\n",
      "[46]\ttrain-rmse:0.425362\teval-rmse:0.462102\n",
      "[47]\ttrain-rmse:0.419295\teval-rmse:0.457024\n",
      "[48]\ttrain-rmse:0.413203\teval-rmse:0.451904\n",
      "[49]\ttrain-rmse:0.407480\teval-rmse:0.447115\n",
      "[50]\ttrain-rmse:0.401883\teval-rmse:0.442406\n",
      "[51]\ttrain-rmse:0.396171\teval-rmse:0.437665\n",
      "[52]\ttrain-rmse:0.390776\teval-rmse:0.433092\n",
      "[53]\ttrain-rmse:0.385399\teval-rmse:0.428638\n",
      "[54]\ttrain-rmse:0.380134\teval-rmse:0.424307\n",
      "[55]\ttrain-rmse:0.375009\teval-rmse:0.420145\n",
      "[56]\ttrain-rmse:0.370021\teval-rmse:0.416248\n",
      "[57]\ttrain-rmse:0.365170\teval-rmse:0.412210\n",
      "[58]\ttrain-rmse:0.360470\teval-rmse:0.408504\n",
      "[59]\ttrain-rmse:0.355748\teval-rmse:0.404762\n",
      "[60]\ttrain-rmse:0.351248\teval-rmse:0.401177\n",
      "[61]\ttrain-rmse:0.346871\teval-rmse:0.397796\n",
      "[62]\ttrain-rmse:0.342462\teval-rmse:0.394416\n",
      "[63]\ttrain-rmse:0.338226\teval-rmse:0.391074\n",
      "[64]\ttrain-rmse:0.333944\teval-rmse:0.387682\n",
      "[65]\ttrain-rmse:0.329763\teval-rmse:0.384591\n",
      "[66]\ttrain-rmse:0.325882\teval-rmse:0.381642\n",
      "[67]\ttrain-rmse:0.321993\teval-rmse:0.378724\n",
      "[68]\ttrain-rmse:0.318181\teval-rmse:0.375912\n",
      "[69]\ttrain-rmse:0.314446\teval-rmse:0.373087\n",
      "[70]\ttrain-rmse:0.310753\teval-rmse:0.370476\n",
      "[71]\ttrain-rmse:0.307331\teval-rmse:0.367970\n",
      "[72]\ttrain-rmse:0.303920\teval-rmse:0.365464\n",
      "[73]\ttrain-rmse:0.300663\teval-rmse:0.363101\n",
      "[74]\ttrain-rmse:0.297449\teval-rmse:0.360839\n",
      "[75]\ttrain-rmse:0.294207\teval-rmse:0.358580\n",
      "[76]\ttrain-rmse:0.290918\teval-rmse:0.356258\n",
      "[77]\ttrain-rmse:0.287756\teval-rmse:0.353994\n",
      "[78]\ttrain-rmse:0.284696\teval-rmse:0.351900\n",
      "[79]\ttrain-rmse:0.281624\teval-rmse:0.349830\n",
      "[80]\ttrain-rmse:0.278746\teval-rmse:0.347869\n",
      "[81]\ttrain-rmse:0.275950\teval-rmse:0.345968\n",
      "[82]\ttrain-rmse:0.273184\teval-rmse:0.344209\n",
      "[83]\ttrain-rmse:0.270397\teval-rmse:0.342353\n",
      "[84]\ttrain-rmse:0.267702\teval-rmse:0.340548\n",
      "[85]\ttrain-rmse:0.265143\teval-rmse:0.339028\n",
      "[86]\ttrain-rmse:0.262693\teval-rmse:0.337386\n",
      "[87]\ttrain-rmse:0.260352\teval-rmse:0.335887\n",
      "[88]\ttrain-rmse:0.257938\teval-rmse:0.334323\n",
      "[89]\ttrain-rmse:0.255498\teval-rmse:0.332865\n",
      "[90]\ttrain-rmse:0.253245\teval-rmse:0.331304\n",
      "[91]\ttrain-rmse:0.251025\teval-rmse:0.329969\n",
      "[92]\ttrain-rmse:0.248773\teval-rmse:0.328602\n",
      "[93]\ttrain-rmse:0.246739\teval-rmse:0.327362\n",
      "[94]\ttrain-rmse:0.244711\teval-rmse:0.326113\n",
      "[95]\ttrain-rmse:0.242627\teval-rmse:0.324878\n",
      "[96]\ttrain-rmse:0.240619\teval-rmse:0.323744\n",
      "[97]\ttrain-rmse:0.238731\teval-rmse:0.322718\n",
      "[98]\ttrain-rmse:0.236735\teval-rmse:0.321677\n",
      "[99]\ttrain-rmse:0.234786\teval-rmse:0.320621\n",
      "[100]\ttrain-rmse:0.232889\teval-rmse:0.319505\n",
      "[101]\ttrain-rmse:0.231219\teval-rmse:0.318445\n",
      "[102]\ttrain-rmse:0.229446\teval-rmse:0.317527\n",
      "[103]\ttrain-rmse:0.227791\teval-rmse:0.316533\n",
      "[104]\ttrain-rmse:0.226136\teval-rmse:0.315531\n",
      "[105]\ttrain-rmse:0.224490\teval-rmse:0.314626\n",
      "[106]\ttrain-rmse:0.222944\teval-rmse:0.313662\n",
      "[107]\ttrain-rmse:0.221553\teval-rmse:0.312879\n",
      "[108]\ttrain-rmse:0.220115\teval-rmse:0.312182\n",
      "[109]\ttrain-rmse:0.218626\teval-rmse:0.311498\n",
      "[110]\ttrain-rmse:0.217235\teval-rmse:0.310771\n",
      "[111]\ttrain-rmse:0.215883\teval-rmse:0.310039\n",
      "[112]\ttrain-rmse:0.214579\teval-rmse:0.309329\n",
      "[113]\ttrain-rmse:0.213142\teval-rmse:0.308684\n",
      "[114]\ttrain-rmse:0.211733\teval-rmse:0.307944\n",
      "[115]\ttrain-rmse:0.210536\teval-rmse:0.307261\n",
      "[116]\ttrain-rmse:0.209184\teval-rmse:0.306571\n",
      "[117]\ttrain-rmse:0.207942\teval-rmse:0.305985\n",
      "[118]\ttrain-rmse:0.206655\teval-rmse:0.305437\n",
      "[119]\ttrain-rmse:0.205340\teval-rmse:0.304782\n",
      "[120]\ttrain-rmse:0.204039\teval-rmse:0.304224\n",
      "[121]\ttrain-rmse:0.202911\teval-rmse:0.303733\n",
      "[122]\ttrain-rmse:0.201701\teval-rmse:0.303174\n",
      "[123]\ttrain-rmse:0.200492\teval-rmse:0.302595\n",
      "[124]\ttrain-rmse:0.199448\teval-rmse:0.302128\n",
      "[125]\ttrain-rmse:0.198578\teval-rmse:0.301710\n",
      "[126]\ttrain-rmse:0.197509\teval-rmse:0.301188\n",
      "[127]\ttrain-rmse:0.196437\teval-rmse:0.300695\n",
      "[128]\ttrain-rmse:0.195510\teval-rmse:0.300245\n",
      "[129]\ttrain-rmse:0.194443\teval-rmse:0.299858\n",
      "[130]\ttrain-rmse:0.193494\teval-rmse:0.299521\n",
      "[131]\ttrain-rmse:0.192608\teval-rmse:0.299125\n",
      "[132]\ttrain-rmse:0.191672\teval-rmse:0.298787\n",
      "[133]\ttrain-rmse:0.190891\teval-rmse:0.298464\n",
      "[134]\ttrain-rmse:0.189943\teval-rmse:0.298095\n",
      "[135]\ttrain-rmse:0.189093\teval-rmse:0.297766\n",
      "[136]\ttrain-rmse:0.188130\teval-rmse:0.297453\n",
      "[137]\ttrain-rmse:0.187287\teval-rmse:0.297219\n",
      "[138]\ttrain-rmse:0.186469\teval-rmse:0.296869\n",
      "[139]\ttrain-rmse:0.185583\teval-rmse:0.296575\n",
      "[140]\ttrain-rmse:0.184872\teval-rmse:0.296249\n",
      "[141]\ttrain-rmse:0.184017\teval-rmse:0.295948\n",
      "[142]\ttrain-rmse:0.183079\teval-rmse:0.295693\n",
      "[143]\ttrain-rmse:0.182325\teval-rmse:0.295423\n",
      "[144]\ttrain-rmse:0.181414\teval-rmse:0.295122\n",
      "[145]\ttrain-rmse:0.180539\teval-rmse:0.294844\n",
      "[146]\ttrain-rmse:0.179813\teval-rmse:0.294563\n",
      "[147]\ttrain-rmse:0.179194\teval-rmse:0.294355\n",
      "[148]\ttrain-rmse:0.178337\teval-rmse:0.294118\n",
      "[149]\ttrain-rmse:0.177641\teval-rmse:0.293952\n",
      "[150]\ttrain-rmse:0.176838\teval-rmse:0.293761\n",
      "[151]\ttrain-rmse:0.176062\teval-rmse:0.293553\n",
      "[152]\ttrain-rmse:0.175326\teval-rmse:0.293420\n",
      "[153]\ttrain-rmse:0.174662\teval-rmse:0.293277\n",
      "[154]\ttrain-rmse:0.173960\teval-rmse:0.292971\n",
      "[155]\ttrain-rmse:0.173235\teval-rmse:0.292735\n",
      "[156]\ttrain-rmse:0.172524\teval-rmse:0.292558\n",
      "[157]\ttrain-rmse:0.171854\teval-rmse:0.292373\n",
      "[158]\ttrain-rmse:0.171358\teval-rmse:0.292204\n",
      "[159]\ttrain-rmse:0.170646\teval-rmse:0.292030\n",
      "[160]\ttrain-rmse:0.170115\teval-rmse:0.291911\n",
      "[161]\ttrain-rmse:0.169452\teval-rmse:0.291731\n",
      "[162]\ttrain-rmse:0.168845\teval-rmse:0.291559\n",
      "[163]\ttrain-rmse:0.168415\teval-rmse:0.291396\n",
      "[164]\ttrain-rmse:0.167956\teval-rmse:0.291253\n",
      "[165]\ttrain-rmse:0.167465\teval-rmse:0.291151\n",
      "[166]\ttrain-rmse:0.166942\teval-rmse:0.291069\n",
      "[167]\ttrain-rmse:0.166543\teval-rmse:0.290935\n",
      "[168]\ttrain-rmse:0.165956\teval-rmse:0.290743\n",
      "[169]\ttrain-rmse:0.165593\teval-rmse:0.290621\n",
      "[170]\ttrain-rmse:0.164932\teval-rmse:0.290487\n",
      "[171]\ttrain-rmse:0.164395\teval-rmse:0.290285\n",
      "[172]\ttrain-rmse:0.163883\teval-rmse:0.290172\n",
      "[173]\ttrain-rmse:0.163414\teval-rmse:0.290076\n",
      "[174]\ttrain-rmse:0.163056\teval-rmse:0.289966\n",
      "[175]\ttrain-rmse:0.162577\teval-rmse:0.289861\n",
      "[176]\ttrain-rmse:0.162208\teval-rmse:0.289755\n",
      "[177]\ttrain-rmse:0.161725\teval-rmse:0.289630\n",
      "[178]\ttrain-rmse:0.161117\teval-rmse:0.289539\n",
      "[179]\ttrain-rmse:0.160713\teval-rmse:0.289461\n",
      "[180]\ttrain-rmse:0.160162\teval-rmse:0.289407\n",
      "[181]\ttrain-rmse:0.159644\teval-rmse:0.289315\n",
      "[182]\ttrain-rmse:0.159120\teval-rmse:0.289204\n",
      "[183]\ttrain-rmse:0.158749\teval-rmse:0.289071\n",
      "[184]\ttrain-rmse:0.158402\teval-rmse:0.289009\n",
      "[185]\ttrain-rmse:0.157968\teval-rmse:0.288902\n",
      "[186]\ttrain-rmse:0.157540\teval-rmse:0.288797\n",
      "[187]\ttrain-rmse:0.157151\teval-rmse:0.288725\n",
      "[188]\ttrain-rmse:0.156796\teval-rmse:0.288645\n",
      "[189]\ttrain-rmse:0.156263\teval-rmse:0.288470\n",
      "[190]\ttrain-rmse:0.155951\teval-rmse:0.288440\n",
      "[191]\ttrain-rmse:0.155613\teval-rmse:0.288362\n",
      "[192]\ttrain-rmse:0.155281\teval-rmse:0.288238\n",
      "[193]\ttrain-rmse:0.154865\teval-rmse:0.288261\n",
      "[194]\ttrain-rmse:0.154486\teval-rmse:0.288139\n",
      "[195]\ttrain-rmse:0.154086\teval-rmse:0.288114\n",
      "[196]\ttrain-rmse:0.153667\teval-rmse:0.288040\n",
      "[197]\ttrain-rmse:0.153388\teval-rmse:0.288044\n",
      "[198]\ttrain-rmse:0.153021\teval-rmse:0.288000\n",
      "[199]\ttrain-rmse:0.152675\teval-rmse:0.287980\n",
      "[200]\ttrain-rmse:0.152303\teval-rmse:0.287934\n",
      "[201]\ttrain-rmse:0.151926\teval-rmse:0.287874\n",
      "[202]\ttrain-rmse:0.151535\teval-rmse:0.287820\n",
      "[203]\ttrain-rmse:0.151264\teval-rmse:0.287787\n",
      "[204]\ttrain-rmse:0.150942\teval-rmse:0.287756\n",
      "[205]\ttrain-rmse:0.150630\teval-rmse:0.287676\n",
      "[206]\ttrain-rmse:0.150254\teval-rmse:0.287638\n",
      "[207]\ttrain-rmse:0.150013\teval-rmse:0.287633\n",
      "[208]\ttrain-rmse:0.149678\teval-rmse:0.287577\n",
      "[209]\ttrain-rmse:0.149261\teval-rmse:0.287528\n",
      "[210]\ttrain-rmse:0.149028\teval-rmse:0.287549\n",
      "[211]\ttrain-rmse:0.148782\teval-rmse:0.287445\n",
      "[212]\ttrain-rmse:0.148478\teval-rmse:0.287395\n",
      "[213]\ttrain-rmse:0.148060\teval-rmse:0.287406\n",
      "[214]\ttrain-rmse:0.147832\teval-rmse:0.287379\n",
      "[215]\ttrain-rmse:0.147515\teval-rmse:0.287334\n",
      "[216]\ttrain-rmse:0.147255\teval-rmse:0.287301\n",
      "[217]\ttrain-rmse:0.147017\teval-rmse:0.287223\n",
      "[218]\ttrain-rmse:0.146713\teval-rmse:0.287172\n",
      "[219]\ttrain-rmse:0.146500\teval-rmse:0.287190\n",
      "[220]\ttrain-rmse:0.146104\teval-rmse:0.287170\n",
      "[221]\ttrain-rmse:0.145763\teval-rmse:0.287144\n",
      "[222]\ttrain-rmse:0.145612\teval-rmse:0.287085\n",
      "[223]\ttrain-rmse:0.145257\teval-rmse:0.287062\n",
      "[224]\ttrain-rmse:0.145037\teval-rmse:0.286939\n",
      "[225]\ttrain-rmse:0.144780\teval-rmse:0.286923\n",
      "[226]\ttrain-rmse:0.144391\teval-rmse:0.286880\n",
      "[227]\ttrain-rmse:0.144147\teval-rmse:0.286824\n",
      "[228]\ttrain-rmse:0.143872\teval-rmse:0.286747\n",
      "[229]\ttrain-rmse:0.143568\teval-rmse:0.286741\n",
      "[230]\ttrain-rmse:0.143396\teval-rmse:0.286734\n",
      "[231]\ttrain-rmse:0.143116\teval-rmse:0.286617\n",
      "[232]\ttrain-rmse:0.142866\teval-rmse:0.286578\n",
      "[233]\ttrain-rmse:0.142631\teval-rmse:0.286543\n",
      "[234]\ttrain-rmse:0.142266\teval-rmse:0.286457\n",
      "[235]\ttrain-rmse:0.142154\teval-rmse:0.286455\n",
      "[236]\ttrain-rmse:0.141932\teval-rmse:0.286437\n",
      "[237]\ttrain-rmse:0.141626\teval-rmse:0.286407\n",
      "[238]\ttrain-rmse:0.141407\teval-rmse:0.286337\n",
      "[239]\ttrain-rmse:0.141096\teval-rmse:0.286300\n",
      "[240]\ttrain-rmse:0.140775\teval-rmse:0.286284\n",
      "[241]\ttrain-rmse:0.140561\teval-rmse:0.286256\n",
      "[242]\ttrain-rmse:0.140227\teval-rmse:0.286273\n",
      "[243]\ttrain-rmse:0.140012\teval-rmse:0.286238\n",
      "[244]\ttrain-rmse:0.139692\teval-rmse:0.286216\n",
      "[245]\ttrain-rmse:0.139457\teval-rmse:0.286189\n",
      "[246]\ttrain-rmse:0.139297\teval-rmse:0.286163\n",
      "[247]\ttrain-rmse:0.139054\teval-rmse:0.286147\n",
      "[248]\ttrain-rmse:0.138912\teval-rmse:0.286116\n",
      "[249]\ttrain-rmse:0.138711\teval-rmse:0.286124\n",
      "[250]\ttrain-rmse:0.138612\teval-rmse:0.286131\n",
      "[251]\ttrain-rmse:0.138412\teval-rmse:0.286086\n",
      "[252]\ttrain-rmse:0.138208\teval-rmse:0.286030\n",
      "[253]\ttrain-rmse:0.137900\teval-rmse:0.285976\n",
      "[254]\ttrain-rmse:0.137749\teval-rmse:0.285960\n",
      "[255]\ttrain-rmse:0.137485\teval-rmse:0.285915\n",
      "[256]\ttrain-rmse:0.137263\teval-rmse:0.285876\n",
      "[257]\ttrain-rmse:0.137045\teval-rmse:0.285871\n",
      "[258]\ttrain-rmse:0.136832\teval-rmse:0.285860\n",
      "[259]\ttrain-rmse:0.136569\teval-rmse:0.285801\n",
      "[260]\ttrain-rmse:0.136440\teval-rmse:0.285800\n",
      "[261]\ttrain-rmse:0.136216\teval-rmse:0.285751\n",
      "[262]\ttrain-rmse:0.136062\teval-rmse:0.285741\n",
      "[263]\ttrain-rmse:0.135906\teval-rmse:0.285720\n",
      "[264]\ttrain-rmse:0.135577\teval-rmse:0.285633\n",
      "[265]\ttrain-rmse:0.135460\teval-rmse:0.285642\n",
      "[266]\ttrain-rmse:0.135281\teval-rmse:0.285615\n",
      "[267]\ttrain-rmse:0.135162\teval-rmse:0.285594\n",
      "[268]\ttrain-rmse:0.135048\teval-rmse:0.285580\n",
      "[269]\ttrain-rmse:0.134773\teval-rmse:0.285566\n",
      "[270]\ttrain-rmse:0.134546\teval-rmse:0.285622\n",
      "[271]\ttrain-rmse:0.134399\teval-rmse:0.285622\n",
      "[272]\ttrain-rmse:0.134219\teval-rmse:0.285585\n",
      "[273]\ttrain-rmse:0.134135\teval-rmse:0.285565\n",
      "[274]\ttrain-rmse:0.133894\teval-rmse:0.285572\n",
      "[275]\ttrain-rmse:0.133653\teval-rmse:0.285504\n",
      "[276]\ttrain-rmse:0.133431\teval-rmse:0.285480\n",
      "[277]\ttrain-rmse:0.133118\teval-rmse:0.285422\n",
      "[278]\ttrain-rmse:0.132972\teval-rmse:0.285409\n",
      "[279]\ttrain-rmse:0.132856\teval-rmse:0.285397\n",
      "[280]\ttrain-rmse:0.132736\teval-rmse:0.285359\n",
      "[281]\ttrain-rmse:0.132499\teval-rmse:0.285289\n",
      "[282]\ttrain-rmse:0.132360\teval-rmse:0.285327\n",
      "[283]\ttrain-rmse:0.132277\teval-rmse:0.285312\n",
      "[284]\ttrain-rmse:0.132130\teval-rmse:0.285262\n",
      "[285]\ttrain-rmse:0.131957\teval-rmse:0.285252\n",
      "[286]\ttrain-rmse:0.131652\teval-rmse:0.285215\n",
      "[287]\ttrain-rmse:0.131507\teval-rmse:0.285196\n",
      "[288]\ttrain-rmse:0.131279\teval-rmse:0.285167\n",
      "[289]\ttrain-rmse:0.131108\teval-rmse:0.285170\n",
      "[290]\ttrain-rmse:0.130723\teval-rmse:0.285155\n",
      "[291]\ttrain-rmse:0.130541\teval-rmse:0.285171\n",
      "[292]\ttrain-rmse:0.130231\teval-rmse:0.285162\n",
      "[293]\ttrain-rmse:0.130107\teval-rmse:0.285131\n",
      "[294]\ttrain-rmse:0.129934\teval-rmse:0.285118\n",
      "[295]\ttrain-rmse:0.129683\teval-rmse:0.285098\n",
      "[296]\ttrain-rmse:0.129543\teval-rmse:0.285042\n",
      "[297]\ttrain-rmse:0.129387\teval-rmse:0.285028\n",
      "[298]\ttrain-rmse:0.129311\teval-rmse:0.284991\n",
      "[299]\ttrain-rmse:0.129079\teval-rmse:0.284978\n",
      "[300]\ttrain-rmse:0.128878\teval-rmse:0.284977\n",
      "[301]\ttrain-rmse:0.128530\teval-rmse:0.284962\n",
      "[302]\ttrain-rmse:0.128364\teval-rmse:0.284967\n",
      "[303]\ttrain-rmse:0.128117\teval-rmse:0.284946\n",
      "[304]\ttrain-rmse:0.127825\teval-rmse:0.284946\n",
      "[305]\ttrain-rmse:0.127629\teval-rmse:0.284935\n",
      "[306]\ttrain-rmse:0.127493\teval-rmse:0.284937\n",
      "[307]\ttrain-rmse:0.127238\teval-rmse:0.284923\n",
      "[308]\ttrain-rmse:0.127158\teval-rmse:0.284901\n",
      "[309]\ttrain-rmse:0.127058\teval-rmse:0.284887\n",
      "[310]\ttrain-rmse:0.126962\teval-rmse:0.284889\n",
      "[311]\ttrain-rmse:0.126872\teval-rmse:0.284868\n",
      "[312]\ttrain-rmse:0.126592\teval-rmse:0.284876\n",
      "[313]\ttrain-rmse:0.126488\teval-rmse:0.284855\n",
      "[314]\ttrain-rmse:0.126356\teval-rmse:0.284830\n",
      "[315]\ttrain-rmse:0.126156\teval-rmse:0.284798\n",
      "[316]\ttrain-rmse:0.126041\teval-rmse:0.284812\n",
      "[317]\ttrain-rmse:0.125866\teval-rmse:0.284776\n",
      "[318]\ttrain-rmse:0.125756\teval-rmse:0.284784\n",
      "[319]\ttrain-rmse:0.125533\teval-rmse:0.284779\n",
      "[320]\ttrain-rmse:0.125391\teval-rmse:0.284766\n",
      "[321]\ttrain-rmse:0.125154\teval-rmse:0.284696\n",
      "[322]\ttrain-rmse:0.125066\teval-rmse:0.284691\n",
      "[323]\ttrain-rmse:0.124983\teval-rmse:0.284685\n",
      "[324]\ttrain-rmse:0.124843\teval-rmse:0.284718\n",
      "[325]\ttrain-rmse:0.124722\teval-rmse:0.284696\n",
      "[326]\ttrain-rmse:0.124537\teval-rmse:0.284701\n",
      "[327]\ttrain-rmse:0.124223\teval-rmse:0.284651\n",
      "[328]\ttrain-rmse:0.124150\teval-rmse:0.284648\n",
      "[329]\ttrain-rmse:0.123938\teval-rmse:0.284620\n",
      "[330]\ttrain-rmse:0.123850\teval-rmse:0.284613\n",
      "[331]\ttrain-rmse:0.123697\teval-rmse:0.284603\n",
      "[332]\ttrain-rmse:0.123516\teval-rmse:0.284607\n",
      "[333]\ttrain-rmse:0.123086\teval-rmse:0.284627\n",
      "[334]\ttrain-rmse:0.122905\teval-rmse:0.284645\n",
      "[335]\ttrain-rmse:0.122796\teval-rmse:0.284653\n",
      "[336]\ttrain-rmse:0.122630\teval-rmse:0.284643\n",
      "[337]\ttrain-rmse:0.122427\teval-rmse:0.284630\n",
      "[338]\ttrain-rmse:0.122253\teval-rmse:0.284633\n",
      "[339]\ttrain-rmse:0.122096\teval-rmse:0.284619\n",
      "[340]\ttrain-rmse:0.121876\teval-rmse:0.284590\n",
      "[341]\ttrain-rmse:0.121764\teval-rmse:0.284573\n",
      "[342]\ttrain-rmse:0.121646\teval-rmse:0.284564\n",
      "[343]\ttrain-rmse:0.121475\teval-rmse:0.284559\n",
      "[344]\ttrain-rmse:0.121349\teval-rmse:0.284567\n",
      "[345]\ttrain-rmse:0.121112\teval-rmse:0.284576\n",
      "[346]\ttrain-rmse:0.120926\teval-rmse:0.284586\n",
      "[347]\ttrain-rmse:0.120735\teval-rmse:0.284595\n",
      "[348]\ttrain-rmse:0.120519\teval-rmse:0.284573\n",
      "[349]\ttrain-rmse:0.120312\teval-rmse:0.284585\n",
      "[350]\ttrain-rmse:0.120071\teval-rmse:0.284575\n",
      "[351]\ttrain-rmse:0.119923\teval-rmse:0.284567\n",
      "[352]\ttrain-rmse:0.119721\teval-rmse:0.284563\n",
      "[353]\ttrain-rmse:0.119641\teval-rmse:0.284574\n",
      "[354]\ttrain-rmse:0.119473\teval-rmse:0.284582\n",
      "[355]\ttrain-rmse:0.119349\teval-rmse:0.284575\n",
      "[356]\ttrain-rmse:0.119231\teval-rmse:0.284561\n",
      "[357]\ttrain-rmse:0.119111\teval-rmse:0.284566\n",
      "[358]\ttrain-rmse:0.118928\teval-rmse:0.284537\n",
      "[359]\ttrain-rmse:0.118761\teval-rmse:0.284525\n",
      "[360]\ttrain-rmse:0.118659\teval-rmse:0.284509\n",
      "[361]\ttrain-rmse:0.118526\teval-rmse:0.284489\n",
      "[362]\ttrain-rmse:0.118317\teval-rmse:0.284497\n",
      "[363]\ttrain-rmse:0.118129\teval-rmse:0.284503\n",
      "[364]\ttrain-rmse:0.117855\teval-rmse:0.284520\n",
      "[365]\ttrain-rmse:0.117811\teval-rmse:0.284517\n",
      "[366]\ttrain-rmse:0.117712\teval-rmse:0.284492\n",
      "[367]\ttrain-rmse:0.117499\teval-rmse:0.284492\n",
      "[368]\ttrain-rmse:0.117382\teval-rmse:0.284504\n",
      "[369]\ttrain-rmse:0.117199\teval-rmse:0.284501\n",
      "[370]\ttrain-rmse:0.117015\teval-rmse:0.284493\n",
      "[371]\ttrain-rmse:0.116834\teval-rmse:0.284511\n",
      "[372]\ttrain-rmse:0.116688\teval-rmse:0.284513\n",
      "[373]\ttrain-rmse:0.116566\teval-rmse:0.284493\n",
      "[374]\ttrain-rmse:0.116343\teval-rmse:0.284463\n",
      "[375]\ttrain-rmse:0.116262\teval-rmse:0.284467\n",
      "[376]\ttrain-rmse:0.116201\teval-rmse:0.284471\n",
      "[377]\ttrain-rmse:0.116106\teval-rmse:0.284473\n",
      "[378]\ttrain-rmse:0.115734\teval-rmse:0.284470\n",
      "[379]\ttrain-rmse:0.115586\teval-rmse:0.284463\n",
      "[380]\ttrain-rmse:0.115417\teval-rmse:0.284458\n",
      "[381]\ttrain-rmse:0.115284\teval-rmse:0.284492\n",
      "[382]\ttrain-rmse:0.115144\teval-rmse:0.284483\n",
      "[383]\ttrain-rmse:0.114998\teval-rmse:0.284467\n",
      "[384]\ttrain-rmse:0.114888\teval-rmse:0.284447\n",
      "[385]\ttrain-rmse:0.114804\teval-rmse:0.284468\n",
      "[386]\ttrain-rmse:0.114666\teval-rmse:0.284478\n",
      "[387]\ttrain-rmse:0.114493\teval-rmse:0.284466\n",
      "[388]\ttrain-rmse:0.114367\teval-rmse:0.284470\n",
      "[389]\ttrain-rmse:0.114214\teval-rmse:0.284447\n",
      "[390]\ttrain-rmse:0.114074\teval-rmse:0.284432\n",
      "[391]\ttrain-rmse:0.113849\teval-rmse:0.284454\n",
      "[392]\ttrain-rmse:0.113756\teval-rmse:0.284452\n",
      "[393]\ttrain-rmse:0.113619\teval-rmse:0.284421\n",
      "[394]\ttrain-rmse:0.113491\teval-rmse:0.284405\n",
      "[395]\ttrain-rmse:0.113306\teval-rmse:0.284417\n",
      "[396]\ttrain-rmse:0.113114\teval-rmse:0.284430\n",
      "[397]\ttrain-rmse:0.113000\teval-rmse:0.284456\n",
      "[398]\ttrain-rmse:0.112898\teval-rmse:0.284435\n",
      "[399]\ttrain-rmse:0.112828\teval-rmse:0.284429\n",
      "[400]\ttrain-rmse:0.112774\teval-rmse:0.284406\n",
      "[401]\ttrain-rmse:0.112617\teval-rmse:0.284406\n",
      "[402]\ttrain-rmse:0.112474\teval-rmse:0.284401\n",
      "[403]\ttrain-rmse:0.112336\teval-rmse:0.284383\n",
      "[404]\ttrain-rmse:0.112143\teval-rmse:0.284397\n",
      "[405]\ttrain-rmse:0.112018\teval-rmse:0.284393\n",
      "[406]\ttrain-rmse:0.111958\teval-rmse:0.284407\n",
      "[407]\ttrain-rmse:0.111914\teval-rmse:0.284413\n",
      "[408]\ttrain-rmse:0.111869\teval-rmse:0.284411\n",
      "[409]\ttrain-rmse:0.111745\teval-rmse:0.284410\n",
      "[410]\ttrain-rmse:0.111578\teval-rmse:0.284385\n",
      "[411]\ttrain-rmse:0.111486\teval-rmse:0.284366\n",
      "[412]\ttrain-rmse:0.111396\teval-rmse:0.284369\n",
      "[413]\ttrain-rmse:0.111246\teval-rmse:0.284317\n",
      "[414]\ttrain-rmse:0.111003\teval-rmse:0.284311\n",
      "[415]\ttrain-rmse:0.110871\teval-rmse:0.284323\n",
      "[416]\ttrain-rmse:0.110633\teval-rmse:0.284284\n",
      "[417]\ttrain-rmse:0.110404\teval-rmse:0.284280\n",
      "[418]\ttrain-rmse:0.110200\teval-rmse:0.284294\n",
      "[419]\ttrain-rmse:0.110022\teval-rmse:0.284275\n",
      "[420]\ttrain-rmse:0.109942\teval-rmse:0.284279\n",
      "[421]\ttrain-rmse:0.109772\teval-rmse:0.284235\n",
      "[422]\ttrain-rmse:0.109622\teval-rmse:0.284236\n",
      "[423]\ttrain-rmse:0.109407\teval-rmse:0.284209\n",
      "[424]\ttrain-rmse:0.109292\teval-rmse:0.284182\n",
      "[425]\ttrain-rmse:0.109211\teval-rmse:0.284166\n",
      "[426]\ttrain-rmse:0.109149\teval-rmse:0.284166\n",
      "[427]\ttrain-rmse:0.108970\teval-rmse:0.284166\n",
      "[428]\ttrain-rmse:0.108780\teval-rmse:0.284185\n",
      "[429]\ttrain-rmse:0.108613\teval-rmse:0.284154\n",
      "[430]\ttrain-rmse:0.108523\teval-rmse:0.284136\n",
      "[431]\ttrain-rmse:0.108457\teval-rmse:0.284126\n",
      "[432]\ttrain-rmse:0.108193\teval-rmse:0.284132\n",
      "[433]\ttrain-rmse:0.108080\teval-rmse:0.284127\n",
      "[434]\ttrain-rmse:0.107940\teval-rmse:0.284130\n",
      "[435]\ttrain-rmse:0.107804\teval-rmse:0.284107\n",
      "[436]\ttrain-rmse:0.107768\teval-rmse:0.284111\n",
      "[437]\ttrain-rmse:0.107614\teval-rmse:0.284090\n",
      "[438]\ttrain-rmse:0.107488\teval-rmse:0.284083\n",
      "[439]\ttrain-rmse:0.107408\teval-rmse:0.284065\n",
      "[440]\ttrain-rmse:0.107149\teval-rmse:0.284061\n",
      "[441]\ttrain-rmse:0.107045\teval-rmse:0.284060\n",
      "[442]\ttrain-rmse:0.106935\teval-rmse:0.284026\n",
      "[443]\ttrain-rmse:0.106784\teval-rmse:0.284022\n",
      "[444]\ttrain-rmse:0.106670\teval-rmse:0.283994\n",
      "[445]\ttrain-rmse:0.106538\teval-rmse:0.283976\n",
      "[446]\ttrain-rmse:0.106340\teval-rmse:0.283985\n",
      "[447]\ttrain-rmse:0.106203\teval-rmse:0.283988\n",
      "[448]\ttrain-rmse:0.106156\teval-rmse:0.283990\n",
      "[449]\ttrain-rmse:0.106099\teval-rmse:0.283984\n",
      "[450]\ttrain-rmse:0.105917\teval-rmse:0.283988\n",
      "[451]\ttrain-rmse:0.105751\teval-rmse:0.283996\n",
      "[452]\ttrain-rmse:0.105561\teval-rmse:0.283970\n",
      "[453]\ttrain-rmse:0.105355\teval-rmse:0.283982\n",
      "[454]\ttrain-rmse:0.105121\teval-rmse:0.284032\n",
      "[455]\ttrain-rmse:0.105071\teval-rmse:0.284029\n",
      "[456]\ttrain-rmse:0.105001\teval-rmse:0.284038\n",
      "[457]\ttrain-rmse:0.104944\teval-rmse:0.284021\n",
      "[458]\ttrain-rmse:0.104741\teval-rmse:0.283984\n",
      "[459]\ttrain-rmse:0.104518\teval-rmse:0.283972\n",
      "[460]\ttrain-rmse:0.104227\teval-rmse:0.283952\n",
      "[461]\ttrain-rmse:0.104169\teval-rmse:0.283931\n",
      "[462]\ttrain-rmse:0.104011\teval-rmse:0.283913\n",
      "[463]\ttrain-rmse:0.103849\teval-rmse:0.283920\n",
      "[464]\ttrain-rmse:0.103714\teval-rmse:0.283915\n",
      "[465]\ttrain-rmse:0.103488\teval-rmse:0.283952\n",
      "[466]\ttrain-rmse:0.103438\teval-rmse:0.283944\n",
      "[467]\ttrain-rmse:0.103220\teval-rmse:0.283927\n",
      "[468]\ttrain-rmse:0.103163\teval-rmse:0.283938\n",
      "[469]\ttrain-rmse:0.103068\teval-rmse:0.283929\n",
      "[470]\ttrain-rmse:0.102982\teval-rmse:0.283913\n",
      "[471]\ttrain-rmse:0.102905\teval-rmse:0.283914\n",
      "[472]\ttrain-rmse:0.102627\teval-rmse:0.283926\n",
      "[473]\ttrain-rmse:0.102561\teval-rmse:0.283902\n",
      "[474]\ttrain-rmse:0.102519\teval-rmse:0.283896\n",
      "[475]\ttrain-rmse:0.102411\teval-rmse:0.283897\n",
      "[476]\ttrain-rmse:0.102368\teval-rmse:0.283893\n",
      "[477]\ttrain-rmse:0.102313\teval-rmse:0.283881\n",
      "[478]\ttrain-rmse:0.102218\teval-rmse:0.283878\n",
      "[479]\ttrain-rmse:0.102071\teval-rmse:0.283881\n",
      "[480]\ttrain-rmse:0.101955\teval-rmse:0.283912\n",
      "[481]\ttrain-rmse:0.101869\teval-rmse:0.283915\n",
      "[482]\ttrain-rmse:0.101679\teval-rmse:0.283917\n",
      "[483]\ttrain-rmse:0.101617\teval-rmse:0.283908\n",
      "[484]\ttrain-rmse:0.101467\teval-rmse:0.283904\n",
      "[485]\ttrain-rmse:0.101284\teval-rmse:0.283892\n",
      "[486]\ttrain-rmse:0.101144\teval-rmse:0.283870\n",
      "[487]\ttrain-rmse:0.101010\teval-rmse:0.283873\n",
      "[488]\ttrain-rmse:0.100893\teval-rmse:0.283866\n",
      "[489]\ttrain-rmse:0.100875\teval-rmse:0.283863\n",
      "[490]\ttrain-rmse:0.100667\teval-rmse:0.283855\n",
      "[491]\ttrain-rmse:0.100577\teval-rmse:0.283852\n",
      "[492]\ttrain-rmse:0.100521\teval-rmse:0.283855\n",
      "[493]\ttrain-rmse:0.100399\teval-rmse:0.283874\n",
      "[494]\ttrain-rmse:0.100257\teval-rmse:0.283885\n",
      "[495]\ttrain-rmse:0.100047\teval-rmse:0.283882\n",
      "[496]\ttrain-rmse:0.099985\teval-rmse:0.283873\n",
      "[497]\ttrain-rmse:0.099926\teval-rmse:0.283862\n",
      "[498]\ttrain-rmse:0.099752\teval-rmse:0.283882\n",
      "[499]\ttrain-rmse:0.099722\teval-rmse:0.283872\n",
      "[500]\ttrain-rmse:0.099627\teval-rmse:0.283887\n",
      "[501]\ttrain-rmse:0.099563\teval-rmse:0.283878\n",
      "[502]\ttrain-rmse:0.099532\teval-rmse:0.283882\n",
      "[503]\ttrain-rmse:0.099423\teval-rmse:0.283879\n",
      "[504]\ttrain-rmse:0.099214\teval-rmse:0.283865\n",
      "[505]\ttrain-rmse:0.099192\teval-rmse:0.283865\n",
      "[506]\ttrain-rmse:0.099069\teval-rmse:0.283849\n",
      "[507]\ttrain-rmse:0.098978\teval-rmse:0.283864\n",
      "[508]\ttrain-rmse:0.098845\teval-rmse:0.283865\n",
      "[509]\ttrain-rmse:0.098686\teval-rmse:0.283861\n",
      "[510]\ttrain-rmse:0.098581\teval-rmse:0.283854\n",
      "[511]\ttrain-rmse:0.098527\teval-rmse:0.283851\n",
      "[512]\ttrain-rmse:0.098465\teval-rmse:0.283833\n",
      "[513]\ttrain-rmse:0.098339\teval-rmse:0.283837\n",
      "[514]\ttrain-rmse:0.098315\teval-rmse:0.283827\n",
      "[515]\ttrain-rmse:0.098200\teval-rmse:0.283820\n",
      "[516]\ttrain-rmse:0.098070\teval-rmse:0.283804\n",
      "[517]\ttrain-rmse:0.097947\teval-rmse:0.283819\n",
      "[518]\ttrain-rmse:0.097922\teval-rmse:0.283822\n",
      "[519]\ttrain-rmse:0.097811\teval-rmse:0.283834\n",
      "[520]\ttrain-rmse:0.097757\teval-rmse:0.283828\n",
      "[521]\ttrain-rmse:0.097613\teval-rmse:0.283825\n",
      "[522]\ttrain-rmse:0.097409\teval-rmse:0.283810\n",
      "[523]\ttrain-rmse:0.097392\teval-rmse:0.283814\n",
      "[524]\ttrain-rmse:0.097325\teval-rmse:0.283808\n",
      "[525]\ttrain-rmse:0.097234\teval-rmse:0.283794\n",
      "[526]\ttrain-rmse:0.097116\teval-rmse:0.283807\n",
      "[527]\ttrain-rmse:0.096981\teval-rmse:0.283815\n",
      "[528]\ttrain-rmse:0.096874\teval-rmse:0.283800\n",
      "[529]\ttrain-rmse:0.096840\teval-rmse:0.283804\n",
      "[530]\ttrain-rmse:0.096670\teval-rmse:0.283821\n",
      "[531]\ttrain-rmse:0.096509\teval-rmse:0.283813\n",
      "[532]\ttrain-rmse:0.096376\teval-rmse:0.283808\n",
      "[533]\ttrain-rmse:0.096197\teval-rmse:0.283819\n",
      "[534]\ttrain-rmse:0.096101\teval-rmse:0.283805\n",
      "[535]\ttrain-rmse:0.095986\teval-rmse:0.283789\n",
      "[536]\ttrain-rmse:0.095781\teval-rmse:0.283771\n",
      "[537]\ttrain-rmse:0.095586\teval-rmse:0.283780\n",
      "[538]\ttrain-rmse:0.095432\teval-rmse:0.283771\n",
      "[539]\ttrain-rmse:0.095372\teval-rmse:0.283776\n",
      "[540]\ttrain-rmse:0.095185\teval-rmse:0.283810\n",
      "[541]\ttrain-rmse:0.095066\teval-rmse:0.283772\n",
      "[542]\ttrain-rmse:0.094961\teval-rmse:0.283765\n",
      "[543]\ttrain-rmse:0.094798\teval-rmse:0.283753\n",
      "[544]\ttrain-rmse:0.094691\teval-rmse:0.283758\n",
      "[545]\ttrain-rmse:0.094590\teval-rmse:0.283733\n",
      "[546]\ttrain-rmse:0.094494\teval-rmse:0.283737\n",
      "[547]\ttrain-rmse:0.094406\teval-rmse:0.283730\n",
      "[548]\ttrain-rmse:0.094316\teval-rmse:0.283735\n",
      "[549]\ttrain-rmse:0.094211\teval-rmse:0.283739\n",
      "[550]\ttrain-rmse:0.094171\teval-rmse:0.283733\n",
      "[551]\ttrain-rmse:0.094032\teval-rmse:0.283731\n",
      "[552]\ttrain-rmse:0.093972\teval-rmse:0.283722\n",
      "[553]\ttrain-rmse:0.093855\teval-rmse:0.283720\n",
      "[554]\ttrain-rmse:0.093734\teval-rmse:0.283717\n",
      "[555]\ttrain-rmse:0.093582\teval-rmse:0.283725\n",
      "[556]\ttrain-rmse:0.093361\teval-rmse:0.283709\n",
      "[557]\ttrain-rmse:0.093224\teval-rmse:0.283722\n",
      "[558]\ttrain-rmse:0.093115\teval-rmse:0.283731\n",
      "[559]\ttrain-rmse:0.093073\teval-rmse:0.283734\n",
      "[560]\ttrain-rmse:0.092956\teval-rmse:0.283703\n",
      "[561]\ttrain-rmse:0.092803\teval-rmse:0.283713\n",
      "[562]\ttrain-rmse:0.092617\teval-rmse:0.283724\n",
      "[563]\ttrain-rmse:0.092436\teval-rmse:0.283735\n",
      "[564]\ttrain-rmse:0.092368\teval-rmse:0.283740\n",
      "[565]\ttrain-rmse:0.092287\teval-rmse:0.283744\n",
      "[566]\ttrain-rmse:0.092123\teval-rmse:0.283717\n",
      "[567]\ttrain-rmse:0.092023\teval-rmse:0.283714\n",
      "[568]\ttrain-rmse:0.091971\teval-rmse:0.283727\n",
      "[569]\ttrain-rmse:0.091822\teval-rmse:0.283734\n",
      "[570]\ttrain-rmse:0.091788\teval-rmse:0.283741\n",
      "[571]\ttrain-rmse:0.091661\teval-rmse:0.283717\n",
      "[572]\ttrain-rmse:0.091541\teval-rmse:0.283702\n",
      "[573]\ttrain-rmse:0.091450\teval-rmse:0.283700\n",
      "[574]\ttrain-rmse:0.091337\teval-rmse:0.283686\n",
      "[575]\ttrain-rmse:0.091076\teval-rmse:0.283692\n",
      "[576]\ttrain-rmse:0.090918\teval-rmse:0.283698\n",
      "[577]\ttrain-rmse:0.090872\teval-rmse:0.283699\n",
      "[578]\ttrain-rmse:0.090729\teval-rmse:0.283702\n",
      "[579]\ttrain-rmse:0.090536\teval-rmse:0.283706\n",
      "[580]\ttrain-rmse:0.090440\teval-rmse:0.283697\n",
      "[581]\ttrain-rmse:0.090384\teval-rmse:0.283696\n",
      "[582]\ttrain-rmse:0.090238\teval-rmse:0.283694\n",
      "[583]\ttrain-rmse:0.090111\teval-rmse:0.283689\n",
      "[584]\ttrain-rmse:0.090033\teval-rmse:0.283718\n",
      "[585]\ttrain-rmse:0.089891\teval-rmse:0.283674\n",
      "[586]\ttrain-rmse:0.089801\teval-rmse:0.283683\n",
      "[587]\ttrain-rmse:0.089637\teval-rmse:0.283674\n",
      "[588]\ttrain-rmse:0.089583\teval-rmse:0.283668\n",
      "[589]\ttrain-rmse:0.089473\teval-rmse:0.283663\n",
      "[590]\ttrain-rmse:0.089388\teval-rmse:0.283649\n",
      "[591]\ttrain-rmse:0.089292\teval-rmse:0.283657\n",
      "[592]\ttrain-rmse:0.089233\teval-rmse:0.283639\n",
      "[593]\ttrain-rmse:0.089069\teval-rmse:0.283638\n",
      "[594]\ttrain-rmse:0.088983\teval-rmse:0.283636\n",
      "[595]\ttrain-rmse:0.088870\teval-rmse:0.283642\n",
      "[596]\ttrain-rmse:0.088804\teval-rmse:0.283637\n",
      "[597]\ttrain-rmse:0.088707\teval-rmse:0.283645\n",
      "[598]\ttrain-rmse:0.088586\teval-rmse:0.283625\n",
      "[599]\ttrain-rmse:0.088457\teval-rmse:0.283606\n",
      "[600]\ttrain-rmse:0.088436\teval-rmse:0.283607\n",
      "[601]\ttrain-rmse:0.088366\teval-rmse:0.283597\n",
      "[602]\ttrain-rmse:0.088303\teval-rmse:0.283596\n",
      "[603]\ttrain-rmse:0.088196\teval-rmse:0.283596\n",
      "[604]\ttrain-rmse:0.088100\teval-rmse:0.283576\n",
      "[605]\ttrain-rmse:0.088015\teval-rmse:0.283570\n",
      "[606]\ttrain-rmse:0.087833\teval-rmse:0.283553\n",
      "[607]\ttrain-rmse:0.087758\teval-rmse:0.283543\n",
      "[608]\ttrain-rmse:0.087705\teval-rmse:0.283550\n",
      "[609]\ttrain-rmse:0.087651\teval-rmse:0.283550\n",
      "[610]\ttrain-rmse:0.087514\teval-rmse:0.283530\n",
      "[611]\ttrain-rmse:0.087443\teval-rmse:0.283520\n",
      "[612]\ttrain-rmse:0.087378\teval-rmse:0.283519\n",
      "[613]\ttrain-rmse:0.087223\teval-rmse:0.283515\n",
      "[614]\ttrain-rmse:0.087113\teval-rmse:0.283499\n",
      "[615]\ttrain-rmse:0.086981\teval-rmse:0.283529\n",
      "[616]\ttrain-rmse:0.086849\teval-rmse:0.283520\n",
      "[617]\ttrain-rmse:0.086651\teval-rmse:0.283507\n",
      "[618]\ttrain-rmse:0.086565\teval-rmse:0.283536\n",
      "[619]\ttrain-rmse:0.086526\teval-rmse:0.283523\n",
      "[620]\ttrain-rmse:0.086343\teval-rmse:0.283526\n",
      "[621]\ttrain-rmse:0.086281\teval-rmse:0.283516\n",
      "[622]\ttrain-rmse:0.086111\teval-rmse:0.283520\n",
      "[623]\ttrain-rmse:0.086063\teval-rmse:0.283511\n",
      "[624]\ttrain-rmse:0.085979\teval-rmse:0.283484\n",
      "[625]\ttrain-rmse:0.085927\teval-rmse:0.283491\n",
      "[626]\ttrain-rmse:0.085847\teval-rmse:0.283501\n",
      "[627]\ttrain-rmse:0.085701\teval-rmse:0.283502\n",
      "[628]\ttrain-rmse:0.085589\teval-rmse:0.283519\n",
      "[629]\ttrain-rmse:0.085518\teval-rmse:0.283516\n",
      "[630]\ttrain-rmse:0.085386\teval-rmse:0.283487\n",
      "[631]\ttrain-rmse:0.085211\teval-rmse:0.283484\n",
      "[632]\ttrain-rmse:0.085147\teval-rmse:0.283497\n",
      "[633]\ttrain-rmse:0.085044\teval-rmse:0.283485\n",
      "[634]\ttrain-rmse:0.084941\teval-rmse:0.283484\n",
      "[635]\ttrain-rmse:0.084811\teval-rmse:0.283471\n",
      "[636]\ttrain-rmse:0.084679\teval-rmse:0.283468\n",
      "[637]\ttrain-rmse:0.084564\teval-rmse:0.283450\n",
      "[638]\ttrain-rmse:0.084446\teval-rmse:0.283456\n",
      "[639]\ttrain-rmse:0.084269\teval-rmse:0.283456\n",
      "[640]\ttrain-rmse:0.084219\teval-rmse:0.283454\n",
      "[641]\ttrain-rmse:0.084092\teval-rmse:0.283470\n",
      "[642]\ttrain-rmse:0.083991\teval-rmse:0.283467\n",
      "[643]\ttrain-rmse:0.083888\teval-rmse:0.283450\n",
      "[644]\ttrain-rmse:0.083801\teval-rmse:0.283443\n",
      "[645]\ttrain-rmse:0.083754\teval-rmse:0.283436\n",
      "[646]\ttrain-rmse:0.083560\teval-rmse:0.283402\n",
      "[647]\ttrain-rmse:0.083419\teval-rmse:0.283409\n",
      "[648]\ttrain-rmse:0.083272\teval-rmse:0.283404\n",
      "[649]\ttrain-rmse:0.083199\teval-rmse:0.283395\n",
      "[650]\ttrain-rmse:0.083173\teval-rmse:0.283403\n",
      "[651]\ttrain-rmse:0.083118\teval-rmse:0.283396\n",
      "[652]\ttrain-rmse:0.083051\teval-rmse:0.283399\n",
      "[653]\ttrain-rmse:0.082998\teval-rmse:0.283387\n",
      "[654]\ttrain-rmse:0.082842\teval-rmse:0.283393\n",
      "[655]\ttrain-rmse:0.082802\teval-rmse:0.283388\n",
      "[656]\ttrain-rmse:0.082632\teval-rmse:0.283374\n",
      "[657]\ttrain-rmse:0.082431\teval-rmse:0.283353\n",
      "[658]\ttrain-rmse:0.082400\teval-rmse:0.283350\n",
      "[659]\ttrain-rmse:0.082264\teval-rmse:0.283357\n",
      "[660]\ttrain-rmse:0.082186\teval-rmse:0.283350\n",
      "[661]\ttrain-rmse:0.082104\teval-rmse:0.283360\n",
      "[662]\ttrain-rmse:0.082040\teval-rmse:0.283348\n",
      "[663]\ttrain-rmse:0.081884\teval-rmse:0.283352\n",
      "[664]\ttrain-rmse:0.081777\teval-rmse:0.283369\n",
      "[665]\ttrain-rmse:0.081684\teval-rmse:0.283361\n",
      "[666]\ttrain-rmse:0.081549\teval-rmse:0.283367\n",
      "[667]\ttrain-rmse:0.081499\teval-rmse:0.283390\n",
      "[668]\ttrain-rmse:0.081446\teval-rmse:0.283380\n",
      "[669]\ttrain-rmse:0.081234\teval-rmse:0.283374\n",
      "[670]\ttrain-rmse:0.081157\teval-rmse:0.283361\n",
      "[671]\ttrain-rmse:0.081065\teval-rmse:0.283356\n",
      "[672]\ttrain-rmse:0.080954\teval-rmse:0.283350\n",
      "[673]\ttrain-rmse:0.080863\teval-rmse:0.283335\n",
      "[674]\ttrain-rmse:0.080773\teval-rmse:0.283333\n",
      "[675]\ttrain-rmse:0.080656\teval-rmse:0.283347\n",
      "[676]\ttrain-rmse:0.080488\teval-rmse:0.283329\n",
      "[677]\ttrain-rmse:0.080450\teval-rmse:0.283330\n",
      "[678]\ttrain-rmse:0.080329\teval-rmse:0.283325\n",
      "[679]\ttrain-rmse:0.080245\teval-rmse:0.283317\n",
      "[680]\ttrain-rmse:0.080186\teval-rmse:0.283304\n",
      "[681]\ttrain-rmse:0.080129\teval-rmse:0.283298\n",
      "[682]\ttrain-rmse:0.079977\teval-rmse:0.283296\n",
      "[683]\ttrain-rmse:0.079888\teval-rmse:0.283290\n",
      "[684]\ttrain-rmse:0.079862\teval-rmse:0.283293\n",
      "[685]\ttrain-rmse:0.079749\teval-rmse:0.283293\n",
      "[686]\ttrain-rmse:0.079702\teval-rmse:0.283292\n",
      "[687]\ttrain-rmse:0.079639\teval-rmse:0.283279\n",
      "[688]\ttrain-rmse:0.079533\teval-rmse:0.283284\n",
      "[689]\ttrain-rmse:0.079422\teval-rmse:0.283294\n",
      "[690]\ttrain-rmse:0.079321\teval-rmse:0.283289\n",
      "[691]\ttrain-rmse:0.079193\teval-rmse:0.283294\n",
      "[692]\ttrain-rmse:0.079065\teval-rmse:0.283297\n",
      "[693]\ttrain-rmse:0.078994\teval-rmse:0.283300\n",
      "[694]\ttrain-rmse:0.078912\teval-rmse:0.283296\n",
      "[695]\ttrain-rmse:0.078785\teval-rmse:0.283291\n",
      "[696]\ttrain-rmse:0.078674\teval-rmse:0.283275\n",
      "[697]\ttrain-rmse:0.078555\teval-rmse:0.283279\n",
      "[698]\ttrain-rmse:0.078514\teval-rmse:0.283269\n",
      "[699]\ttrain-rmse:0.078495\teval-rmse:0.283265\n",
      "[700]\ttrain-rmse:0.078397\teval-rmse:0.283264\n",
      "[701]\ttrain-rmse:0.078316\teval-rmse:0.283256\n",
      "[702]\ttrain-rmse:0.078263\teval-rmse:0.283254\n",
      "[703]\ttrain-rmse:0.078166\teval-rmse:0.283255\n",
      "[704]\ttrain-rmse:0.078045\teval-rmse:0.283255\n",
      "[705]\ttrain-rmse:0.077948\teval-rmse:0.283247\n",
      "[706]\ttrain-rmse:0.077895\teval-rmse:0.283255\n",
      "[707]\ttrain-rmse:0.077865\teval-rmse:0.283252\n",
      "[708]\ttrain-rmse:0.077798\teval-rmse:0.283265\n",
      "[709]\ttrain-rmse:0.077766\teval-rmse:0.283274\n",
      "[710]\ttrain-rmse:0.077726\teval-rmse:0.283282\n",
      "[711]\ttrain-rmse:0.077661\teval-rmse:0.283287\n",
      "[712]\ttrain-rmse:0.077584\teval-rmse:0.283302\n",
      "[713]\ttrain-rmse:0.077472\teval-rmse:0.283298\n",
      "[714]\ttrain-rmse:0.077408\teval-rmse:0.283294\n",
      "[715]\ttrain-rmse:0.077330\teval-rmse:0.283295\n",
      "[716]\ttrain-rmse:0.077288\teval-rmse:0.283299\n",
      "[717]\ttrain-rmse:0.077201\teval-rmse:0.283297\n",
      "[718]\ttrain-rmse:0.077105\teval-rmse:0.283287\n",
      "[719]\ttrain-rmse:0.077022\teval-rmse:0.283278\n",
      "[720]\ttrain-rmse:0.076948\teval-rmse:0.283282\n",
      "[721]\ttrain-rmse:0.076852\teval-rmse:0.283283\n",
      "[722]\ttrain-rmse:0.076773\teval-rmse:0.283294\n",
      "[723]\ttrain-rmse:0.076654\teval-rmse:0.283320\n",
      "[724]\ttrain-rmse:0.076593\teval-rmse:0.283330\n",
      "[725]\ttrain-rmse:0.076430\teval-rmse:0.283327\n",
      "[726]\ttrain-rmse:0.076330\teval-rmse:0.283335\n",
      "[727]\ttrain-rmse:0.076304\teval-rmse:0.283328\n",
      "[728]\ttrain-rmse:0.076243\teval-rmse:0.283332\n",
      "[729]\ttrain-rmse:0.076201\teval-rmse:0.283326\n",
      "[730]\ttrain-rmse:0.076081\teval-rmse:0.283326\n",
      "[731]\ttrain-rmse:0.076010\teval-rmse:0.283329\n",
      "[732]\ttrain-rmse:0.075903\teval-rmse:0.283331\n",
      "[733]\ttrain-rmse:0.075827\teval-rmse:0.283337\n",
      "[734]\ttrain-rmse:0.075785\teval-rmse:0.283342\n",
      "[735]\ttrain-rmse:0.075654\teval-rmse:0.283339\n",
      "[736]\ttrain-rmse:0.075599\teval-rmse:0.283333\n",
      "[737]\ttrain-rmse:0.075466\teval-rmse:0.283350\n",
      "[738]\ttrain-rmse:0.075416\teval-rmse:0.283357\n",
      "[739]\ttrain-rmse:0.075344\teval-rmse:0.283361\n",
      "[740]\ttrain-rmse:0.075285\teval-rmse:0.283354\n",
      "[741]\ttrain-rmse:0.075247\teval-rmse:0.283347\n",
      "[742]\ttrain-rmse:0.075145\teval-rmse:0.283355\n",
      "[743]\ttrain-rmse:0.075071\teval-rmse:0.283352\n",
      "[744]\ttrain-rmse:0.075006\teval-rmse:0.283345\n",
      "[745]\ttrain-rmse:0.074929\teval-rmse:0.283348\n",
      "[746]\ttrain-rmse:0.074862\teval-rmse:0.283352\n",
      "[747]\ttrain-rmse:0.074757\teval-rmse:0.283351\n",
      "[748]\ttrain-rmse:0.074591\teval-rmse:0.283355\n",
      "[749]\ttrain-rmse:0.074510\teval-rmse:0.283354\n",
      "[750]\ttrain-rmse:0.074322\teval-rmse:0.283341\n",
      "[751]\ttrain-rmse:0.074177\teval-rmse:0.283322\n",
      "[752]\ttrain-rmse:0.074078\teval-rmse:0.283325\n",
      "[753]\ttrain-rmse:0.074002\teval-rmse:0.283324\n",
      "[754]\ttrain-rmse:0.073950\teval-rmse:0.283318\n",
      "[755]\ttrain-rmse:0.073846\teval-rmse:0.283323\n",
      "[756]\ttrain-rmse:0.073735\teval-rmse:0.283339\n",
      "[757]\ttrain-rmse:0.073566\teval-rmse:0.283358\n",
      "[758]\ttrain-rmse:0.073472\teval-rmse:0.283353\n",
      "[759]\ttrain-rmse:0.073433\teval-rmse:0.283350\n",
      "[760]\ttrain-rmse:0.073377\teval-rmse:0.283375\n",
      "[761]\ttrain-rmse:0.073342\teval-rmse:0.283375\n",
      "[762]\ttrain-rmse:0.073276\teval-rmse:0.283370\n",
      "[763]\ttrain-rmse:0.073111\teval-rmse:0.283361\n",
      "[764]\ttrain-rmse:0.073078\teval-rmse:0.283377\n",
      "[765]\ttrain-rmse:0.073033\teval-rmse:0.283370\n",
      "[766]\ttrain-rmse:0.072878\teval-rmse:0.283344\n",
      "[767]\ttrain-rmse:0.072770\teval-rmse:0.283346\n",
      "[768]\ttrain-rmse:0.072723\teval-rmse:0.283352\n",
      "[769]\ttrain-rmse:0.072692\teval-rmse:0.283352\n",
      "[770]\ttrain-rmse:0.072534\teval-rmse:0.283355\n",
      "[771]\ttrain-rmse:0.072472\teval-rmse:0.283350\n",
      "[772]\ttrain-rmse:0.072388\teval-rmse:0.283343\n",
      "[773]\ttrain-rmse:0.072346\teval-rmse:0.283342\n",
      "[774]\ttrain-rmse:0.072320\teval-rmse:0.283337\n",
      "[775]\ttrain-rmse:0.072224\teval-rmse:0.283319\n",
      "[776]\ttrain-rmse:0.072096\teval-rmse:0.283315\n",
      "[777]\ttrain-rmse:0.072029\teval-rmse:0.283312\n",
      "[778]\ttrain-rmse:0.071894\teval-rmse:0.283309\n",
      "[779]\ttrain-rmse:0.071734\teval-rmse:0.283305\n",
      "[780]\ttrain-rmse:0.071598\teval-rmse:0.283275\n",
      "[781]\ttrain-rmse:0.071541\teval-rmse:0.283266\n",
      "[782]\ttrain-rmse:0.071516\teval-rmse:0.283280\n",
      "[783]\ttrain-rmse:0.071421\teval-rmse:0.283286\n",
      "[784]\ttrain-rmse:0.071325\teval-rmse:0.283302\n",
      "[785]\ttrain-rmse:0.071165\teval-rmse:0.283314\n",
      "[786]\ttrain-rmse:0.071121\teval-rmse:0.283297\n",
      "[787]\ttrain-rmse:0.071058\teval-rmse:0.283289\n",
      "[788]\ttrain-rmse:0.070968\teval-rmse:0.283272\n",
      "[789]\ttrain-rmse:0.070868\teval-rmse:0.283277\n",
      "[790]\ttrain-rmse:0.070815\teval-rmse:0.283281\n",
      "[791]\ttrain-rmse:0.070803\teval-rmse:0.283283\n",
      "[792]\ttrain-rmse:0.070667\teval-rmse:0.283288\n",
      "[793]\ttrain-rmse:0.070558\teval-rmse:0.283298\n",
      "[794]\ttrain-rmse:0.070490\teval-rmse:0.283292\n",
      "[795]\ttrain-rmse:0.070334\teval-rmse:0.283306\n",
      "[796]\ttrain-rmse:0.070261\teval-rmse:0.283297\n",
      "[797]\ttrain-rmse:0.070248\teval-rmse:0.283298\n",
      "[798]\ttrain-rmse:0.070163\teval-rmse:0.283302\n",
      "[799]\ttrain-rmse:0.070051\teval-rmse:0.283290\n",
      "[800]\ttrain-rmse:0.069981\teval-rmse:0.283301\n",
      "[801]\ttrain-rmse:0.069875\teval-rmse:0.283314\n",
      "[802]\ttrain-rmse:0.069844\teval-rmse:0.283315\n",
      "[803]\ttrain-rmse:0.069710\teval-rmse:0.283325\n",
      "[804]\ttrain-rmse:0.069662\teval-rmse:0.283321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "RMSE: 0.889242\n",
      "(0.80741511314866665, 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[805]\ttrain-rmse:0.069522\teval-rmse:0.283328\n",
      "Stopping. Best iteration:\n",
      "[705]\ttrain-rmse:0.077948\teval-rmse:0.283247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_valid_old = traintill2014[feat3+['Score']].to_dataframe()\n",
    "gbm_old = train_xgboost(train_valid_old, feat3, num_boost_round=5000, eta=0.02, max_depth=8, test_size=0.2)\n",
    "predictions_old = xgboost_predict(gbm_old, test2015[feat3].to_dataframe(), feat3)\n",
    "print sp.stats.pearsonr(np.array(predictions_old), np.array(test2015['Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Boosted trees regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 13597\n",
      "PROGRESS: Number of features          : 104\n",
      "PROGRESS: Number of unpacked features : 104\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.190662     | 4.30428            | 2.11016       |\n",
      "PROGRESS: | 2         | 0.258727     | 4.14215            | 1.61343       |\n",
      "PROGRESS: | 3         | 0.326030     | 4.14138            | 1.29205       |\n",
      "PROGRESS: | 4         | 0.391642     | 3.86261            | 1.09126       |\n",
      "PROGRESS: | 5         | 0.462069     | 3.83338            | 0.968893      |\n",
      "PROGRESS: | 6         | 0.528827     | 3.87607            | 0.893777      |\n",
      "PROGRESS: | 7         | 0.596910     | 3.8455             | 0.848049      |\n",
      "PROGRESS: | 8         | 0.662952     | 3.7231             | 0.818215      |\n",
      "PROGRESS: | 9         | 0.729395     | 3.68251            | 0.797127      |\n",
      "PROGRESS: | 10        | 0.792390     | 3.6991             | 0.781429      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 13597\n",
      "PROGRESS: Number of features          : 102\n",
      "PROGRESS: Number of unpacked features : 102\n",
      "PROGRESS: Number of coefficients    : 103\n",
      "PROGRESS: Starting Newton Method\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 2        | 0.107971     | 5.160859           | 0.918677      |\n",
      "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n"
     ]
    }
   ],
   "source": [
    "m1 = graphlab.boosted_trees_regression.create(sts_train, target='Score', features=feat2, validation_set=None)\n",
    "\n",
    "m3 = graphlab.linear_regression.create(sts_train, target='Score', features=feat3, validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmse:0.895630\teval-rmse:0.901163\n",
      "[1]\ttrain-rmse:0.879518\teval-rmse:0.885366\n",
      "[2]\ttrain-rmse:0.863749\teval-rmse:0.870009\n",
      "[3]\ttrain-rmse:0.848351\teval-rmse:0.855045\n",
      "[4]\ttrain-rmse:0.833195\teval-rmse:0.840375\n",
      "[5]\ttrain-rmse:0.818631\teval-rmse:0.826247\n",
      "[6]\ttrain-rmse:0.804404\teval-rmse:0.812432\n",
      "[7]\ttrain-rmse:0.790253\teval-rmse:0.798677\n",
      "[8]\ttrain-rmse:0.776276\teval-rmse:0.785222\n",
      "[9]\ttrain-rmse:0.762828\teval-rmse:0.772404\n",
      "[10]\ttrain-rmse:0.749707\teval-rmse:0.759851\n",
      "[11]\ttrain-rmse:0.737007\teval-rmse:0.747672\n",
      "[12]\ttrain-rmse:0.724451\teval-rmse:0.735693\n",
      "[13]\ttrain-rmse:0.711905\teval-rmse:0.723700\n",
      "[14]\ttrain-rmse:0.699669\teval-rmse:0.712023\n",
      "[15]\ttrain-rmse:0.687705\teval-rmse:0.700584\n",
      "[16]\ttrain-rmse:0.676168\teval-rmse:0.689530\n",
      "[17]\ttrain-rmse:0.664648\teval-rmse:0.678573\n",
      "[18]\ttrain-rmse:0.653622\teval-rmse:0.668138\n",
      "[19]\ttrain-rmse:0.642629\teval-rmse:0.657669\n",
      "[20]\ttrain-rmse:0.632102\teval-rmse:0.647650\n",
      "[21]\ttrain-rmse:0.621779\teval-rmse:0.637927\n",
      "[22]\ttrain-rmse:0.611446\teval-rmse:0.628125\n",
      "[23]\ttrain-rmse:0.601348\teval-rmse:0.618693\n",
      "[24]\ttrain-rmse:0.591597\teval-rmse:0.609514\n",
      "[25]\ttrain-rmse:0.582160\teval-rmse:0.600739\n",
      "[26]\ttrain-rmse:0.572682\teval-rmse:0.591918\n",
      "[27]\ttrain-rmse:0.563599\teval-rmse:0.583540\n",
      "[28]\ttrain-rmse:0.554626\teval-rmse:0.575160\n",
      "[29]\ttrain-rmse:0.545977\teval-rmse:0.567163\n",
      "[30]\ttrain-rmse:0.537468\teval-rmse:0.559358\n",
      "[31]\ttrain-rmse:0.529087\teval-rmse:0.551483\n",
      "[32]\ttrain-rmse:0.520831\teval-rmse:0.543891\n",
      "[33]\ttrain-rmse:0.512640\teval-rmse:0.536405\n",
      "[34]\ttrain-rmse:0.504687\teval-rmse:0.529077\n",
      "[35]\ttrain-rmse:0.496939\teval-rmse:0.521948\n",
      "[36]\ttrain-rmse:0.489484\teval-rmse:0.515316\n",
      "[37]\ttrain-rmse:0.482258\teval-rmse:0.508797\n",
      "[38]\ttrain-rmse:0.475229\teval-rmse:0.502507\n",
      "[39]\ttrain-rmse:0.468289\teval-rmse:0.496292\n",
      "[40]\ttrain-rmse:0.461619\teval-rmse:0.490230\n",
      "[41]\ttrain-rmse:0.454822\teval-rmse:0.484130\n",
      "[42]\ttrain-rmse:0.448328\teval-rmse:0.478344\n",
      "[43]\ttrain-rmse:0.441934\teval-rmse:0.472586\n",
      "[44]\ttrain-rmse:0.435727\teval-rmse:0.467041\n",
      "[45]\ttrain-rmse:0.429504\teval-rmse:0.461609\n",
      "[46]\ttrain-rmse:0.423621\teval-rmse:0.456441\n",
      "[47]\ttrain-rmse:0.417791\teval-rmse:0.451436\n",
      "[48]\ttrain-rmse:0.411921\teval-rmse:0.446310\n",
      "[49]\ttrain-rmse:0.406409\teval-rmse:0.441452\n",
      "[50]\ttrain-rmse:0.400858\teval-rmse:0.436680\n",
      "[51]\ttrain-rmse:0.395489\teval-rmse:0.432094\n",
      "[52]\ttrain-rmse:0.390287\teval-rmse:0.427730\n",
      "[53]\ttrain-rmse:0.385123\teval-rmse:0.423306\n",
      "[54]\ttrain-rmse:0.380065\teval-rmse:0.419076\n",
      "[55]\ttrain-rmse:0.375185\teval-rmse:0.414900\n",
      "[56]\ttrain-rmse:0.370413\teval-rmse:0.410902\n",
      "[57]\ttrain-rmse:0.365762\teval-rmse:0.406953\n",
      "[58]\ttrain-rmse:0.361277\teval-rmse:0.403340\n",
      "[59]\ttrain-rmse:0.356870\teval-rmse:0.399648\n",
      "[60]\ttrain-rmse:0.352533\teval-rmse:0.396084\n",
      "[61]\ttrain-rmse:0.348317\teval-rmse:0.392710\n",
      "[62]\ttrain-rmse:0.344187\teval-rmse:0.389269\n",
      "[63]\ttrain-rmse:0.340304\teval-rmse:0.386086\n",
      "[64]\ttrain-rmse:0.336493\teval-rmse:0.383044\n",
      "[65]\ttrain-rmse:0.332628\teval-rmse:0.379995\n",
      "[66]\ttrain-rmse:0.328863\teval-rmse:0.376976\n",
      "[67]\ttrain-rmse:0.325208\teval-rmse:0.374161\n",
      "[68]\ttrain-rmse:0.321708\teval-rmse:0.371426\n",
      "[69]\ttrain-rmse:0.318240\teval-rmse:0.368755\n",
      "[70]\ttrain-rmse:0.314742\teval-rmse:0.366108\n",
      "[71]\ttrain-rmse:0.311356\teval-rmse:0.363481\n",
      "[72]\ttrain-rmse:0.308101\teval-rmse:0.361092\n",
      "[73]\ttrain-rmse:0.304930\teval-rmse:0.358763\n",
      "[74]\ttrain-rmse:0.301784\teval-rmse:0.356379\n",
      "[75]\ttrain-rmse:0.298852\teval-rmse:0.354117\n",
      "[76]\ttrain-rmse:0.295914\teval-rmse:0.351885\n",
      "[77]\ttrain-rmse:0.293060\teval-rmse:0.349735\n",
      "[78]\ttrain-rmse:0.290233\teval-rmse:0.347662\n",
      "[79]\ttrain-rmse:0.287506\teval-rmse:0.345638\n",
      "[80]\ttrain-rmse:0.284728\teval-rmse:0.343682\n",
      "[81]\ttrain-rmse:0.282155\teval-rmse:0.341756\n",
      "[82]\ttrain-rmse:0.279660\teval-rmse:0.339920\n",
      "[83]\ttrain-rmse:0.277256\teval-rmse:0.338206\n",
      "[84]\ttrain-rmse:0.274873\teval-rmse:0.336498\n",
      "[85]\ttrain-rmse:0.272498\teval-rmse:0.334850\n",
      "[86]\ttrain-rmse:0.270171\teval-rmse:0.333306\n",
      "[87]\ttrain-rmse:0.267956\teval-rmse:0.331820\n",
      "[88]\ttrain-rmse:0.265724\teval-rmse:0.330312\n",
      "[89]\ttrain-rmse:0.263549\teval-rmse:0.328876\n",
      "[90]\ttrain-rmse:0.261510\teval-rmse:0.327526\n",
      "[91]\ttrain-rmse:0.259519\teval-rmse:0.326187\n",
      "[92]\ttrain-rmse:0.257502\teval-rmse:0.324874\n",
      "[93]\ttrain-rmse:0.255497\teval-rmse:0.323579\n",
      "[94]\ttrain-rmse:0.253528\teval-rmse:0.322348\n",
      "[95]\ttrain-rmse:0.251703\teval-rmse:0.321163\n",
      "[96]\ttrain-rmse:0.249869\teval-rmse:0.319929\n",
      "[97]\ttrain-rmse:0.248110\teval-rmse:0.318758\n",
      "[98]\ttrain-rmse:0.246342\teval-rmse:0.317724\n",
      "[99]\ttrain-rmse:0.244704\teval-rmse:0.316752\n",
      "[100]\ttrain-rmse:0.243086\teval-rmse:0.315746\n",
      "[101]\ttrain-rmse:0.241472\teval-rmse:0.314788\n",
      "[102]\ttrain-rmse:0.239949\teval-rmse:0.313866\n",
      "[103]\ttrain-rmse:0.238439\teval-rmse:0.312975\n",
      "[104]\ttrain-rmse:0.236875\teval-rmse:0.312050\n",
      "[105]\ttrain-rmse:0.235448\teval-rmse:0.311287\n",
      "[106]\ttrain-rmse:0.234057\teval-rmse:0.310545\n",
      "[107]\ttrain-rmse:0.232594\teval-rmse:0.309662\n",
      "[108]\ttrain-rmse:0.231197\teval-rmse:0.308847\n",
      "[109]\ttrain-rmse:0.229893\teval-rmse:0.308098\n",
      "[110]\ttrain-rmse:0.228589\teval-rmse:0.307411\n",
      "[111]\ttrain-rmse:0.227395\teval-rmse:0.306679\n",
      "[112]\ttrain-rmse:0.226124\teval-rmse:0.305984\n",
      "[113]\ttrain-rmse:0.224843\teval-rmse:0.305287\n",
      "[114]\ttrain-rmse:0.223699\teval-rmse:0.304610\n",
      "[115]\ttrain-rmse:0.222624\teval-rmse:0.303952\n",
      "[116]\ttrain-rmse:0.221543\teval-rmse:0.303304\n",
      "[117]\ttrain-rmse:0.220525\teval-rmse:0.302686\n",
      "[118]\ttrain-rmse:0.219405\teval-rmse:0.302089\n",
      "[119]\ttrain-rmse:0.218263\teval-rmse:0.301502\n",
      "[120]\ttrain-rmse:0.217242\teval-rmse:0.301015\n",
      "[121]\ttrain-rmse:0.216200\teval-rmse:0.300541\n",
      "[122]\ttrain-rmse:0.215209\teval-rmse:0.300108\n",
      "[123]\ttrain-rmse:0.214215\teval-rmse:0.299573\n",
      "[124]\ttrain-rmse:0.213272\teval-rmse:0.299173\n",
      "[125]\ttrain-rmse:0.212326\teval-rmse:0.298761\n",
      "[126]\ttrain-rmse:0.211410\teval-rmse:0.298314\n",
      "[127]\ttrain-rmse:0.210568\teval-rmse:0.297837\n",
      "[128]\ttrain-rmse:0.209735\teval-rmse:0.297425\n",
      "[129]\ttrain-rmse:0.208872\teval-rmse:0.297026\n",
      "[130]\ttrain-rmse:0.208048\teval-rmse:0.296650\n",
      "[131]\ttrain-rmse:0.207190\teval-rmse:0.296259\n",
      "[132]\ttrain-rmse:0.206368\teval-rmse:0.295926\n",
      "[133]\ttrain-rmse:0.205541\teval-rmse:0.295522\n",
      "[134]\ttrain-rmse:0.204815\teval-rmse:0.295195\n",
      "[135]\ttrain-rmse:0.204068\teval-rmse:0.294870\n",
      "[136]\ttrain-rmse:0.203297\teval-rmse:0.294574\n",
      "[137]\ttrain-rmse:0.202583\teval-rmse:0.294199\n",
      "[138]\ttrain-rmse:0.201891\teval-rmse:0.293886\n",
      "[139]\ttrain-rmse:0.201229\teval-rmse:0.293616\n",
      "[140]\ttrain-rmse:0.200527\teval-rmse:0.293354\n",
      "[141]\ttrain-rmse:0.199769\teval-rmse:0.293064\n",
      "[142]\ttrain-rmse:0.199176\teval-rmse:0.292784\n",
      "[143]\ttrain-rmse:0.198426\teval-rmse:0.292520\n",
      "[144]\ttrain-rmse:0.197769\teval-rmse:0.292198\n",
      "[145]\ttrain-rmse:0.197258\teval-rmse:0.291946\n",
      "[146]\ttrain-rmse:0.196622\teval-rmse:0.291747\n",
      "[147]\ttrain-rmse:0.195979\teval-rmse:0.291478\n",
      "[148]\ttrain-rmse:0.195407\teval-rmse:0.291208\n",
      "[149]\ttrain-rmse:0.194726\teval-rmse:0.290973\n",
      "[150]\ttrain-rmse:0.194108\teval-rmse:0.290806\n",
      "[151]\ttrain-rmse:0.193515\teval-rmse:0.290618\n",
      "[152]\ttrain-rmse:0.192959\teval-rmse:0.290403\n",
      "[153]\ttrain-rmse:0.192485\teval-rmse:0.290230\n",
      "[154]\ttrain-rmse:0.191998\teval-rmse:0.290075\n",
      "[155]\ttrain-rmse:0.191404\teval-rmse:0.289947\n",
      "[156]\ttrain-rmse:0.190839\teval-rmse:0.289750\n",
      "[157]\ttrain-rmse:0.190349\teval-rmse:0.289594\n",
      "[158]\ttrain-rmse:0.189856\teval-rmse:0.289451\n",
      "[159]\ttrain-rmse:0.189341\teval-rmse:0.289319\n",
      "[160]\ttrain-rmse:0.188791\teval-rmse:0.289199\n",
      "[161]\ttrain-rmse:0.188278\teval-rmse:0.289033\n",
      "[162]\ttrain-rmse:0.187719\teval-rmse:0.288956\n",
      "[163]\ttrain-rmse:0.187234\teval-rmse:0.288844\n",
      "[164]\ttrain-rmse:0.186720\teval-rmse:0.288672\n",
      "[165]\ttrain-rmse:0.186294\teval-rmse:0.288572\n",
      "[166]\ttrain-rmse:0.185850\teval-rmse:0.288459\n",
      "[167]\ttrain-rmse:0.185415\teval-rmse:0.288304\n",
      "[168]\ttrain-rmse:0.184946\teval-rmse:0.288192\n",
      "[169]\ttrain-rmse:0.184550\teval-rmse:0.288079\n",
      "[170]\ttrain-rmse:0.184216\teval-rmse:0.287945\n",
      "[171]\ttrain-rmse:0.183816\teval-rmse:0.287843\n",
      "[172]\ttrain-rmse:0.183326\teval-rmse:0.287750\n",
      "[173]\ttrain-rmse:0.182912\teval-rmse:0.287613\n",
      "[174]\ttrain-rmse:0.182519\teval-rmse:0.287542\n",
      "[175]\ttrain-rmse:0.182078\teval-rmse:0.287492\n",
      "[176]\ttrain-rmse:0.181694\teval-rmse:0.287376\n",
      "[177]\ttrain-rmse:0.181221\teval-rmse:0.287270\n",
      "[178]\ttrain-rmse:0.180932\teval-rmse:0.287167\n",
      "[179]\ttrain-rmse:0.180569\teval-rmse:0.287116\n",
      "[180]\ttrain-rmse:0.180219\teval-rmse:0.287013\n",
      "[181]\ttrain-rmse:0.179815\teval-rmse:0.286941\n",
      "[182]\ttrain-rmse:0.179324\teval-rmse:0.286854\n",
      "[183]\ttrain-rmse:0.178995\teval-rmse:0.286760\n",
      "[184]\ttrain-rmse:0.178651\teval-rmse:0.286688\n",
      "[185]\ttrain-rmse:0.178300\teval-rmse:0.286613\n",
      "[186]\ttrain-rmse:0.178126\teval-rmse:0.286544\n",
      "[187]\ttrain-rmse:0.177652\teval-rmse:0.286489\n",
      "[188]\ttrain-rmse:0.177339\teval-rmse:0.286390\n",
      "[189]\ttrain-rmse:0.176969\teval-rmse:0.286290\n",
      "[190]\ttrain-rmse:0.176602\teval-rmse:0.286242\n",
      "[191]\ttrain-rmse:0.176338\teval-rmse:0.286143\n",
      "[192]\ttrain-rmse:0.176019\teval-rmse:0.286068\n",
      "[193]\ttrain-rmse:0.175697\teval-rmse:0.286001\n",
      "[194]\ttrain-rmse:0.175306\teval-rmse:0.285939\n",
      "[195]\ttrain-rmse:0.174931\teval-rmse:0.285914\n",
      "[196]\ttrain-rmse:0.174637\teval-rmse:0.285871\n",
      "[197]\ttrain-rmse:0.174369\teval-rmse:0.285848\n",
      "[198]\ttrain-rmse:0.173992\teval-rmse:0.285804\n",
      "[199]\ttrain-rmse:0.173694\teval-rmse:0.285746\n",
      "[200]\ttrain-rmse:0.173460\teval-rmse:0.285690\n",
      "[201]\ttrain-rmse:0.173197\teval-rmse:0.285628\n",
      "[202]\ttrain-rmse:0.172886\teval-rmse:0.285561\n",
      "[203]\ttrain-rmse:0.172558\teval-rmse:0.285484\n",
      "[204]\ttrain-rmse:0.172185\teval-rmse:0.285436\n",
      "[205]\ttrain-rmse:0.171883\teval-rmse:0.285393\n",
      "[206]\ttrain-rmse:0.171545\teval-rmse:0.285352\n",
      "[207]\ttrain-rmse:0.171206\teval-rmse:0.285283\n",
      "[208]\ttrain-rmse:0.170897\teval-rmse:0.285227\n",
      "[209]\ttrain-rmse:0.170569\teval-rmse:0.285127\n",
      "[210]\ttrain-rmse:0.170271\teval-rmse:0.285117\n",
      "[211]\ttrain-rmse:0.169942\teval-rmse:0.285020\n",
      "[212]\ttrain-rmse:0.169640\teval-rmse:0.284978\n",
      "[213]\ttrain-rmse:0.169366\teval-rmse:0.284896\n",
      "[214]\ttrain-rmse:0.169056\teval-rmse:0.284815\n",
      "[215]\ttrain-rmse:0.168725\teval-rmse:0.284782\n",
      "[216]\ttrain-rmse:0.168485\teval-rmse:0.284758\n",
      "[217]\ttrain-rmse:0.168345\teval-rmse:0.284717\n",
      "[218]\ttrain-rmse:0.168022\teval-rmse:0.284676\n",
      "[219]\ttrain-rmse:0.167860\teval-rmse:0.284618\n",
      "[220]\ttrain-rmse:0.167647\teval-rmse:0.284608\n",
      "[221]\ttrain-rmse:0.167424\teval-rmse:0.284566\n",
      "[222]\ttrain-rmse:0.167082\teval-rmse:0.284572\n",
      "[223]\ttrain-rmse:0.166841\teval-rmse:0.284538\n",
      "[224]\ttrain-rmse:0.166461\teval-rmse:0.284497\n",
      "[225]\ttrain-rmse:0.166239\teval-rmse:0.284425\n",
      "[226]\ttrain-rmse:0.166054\teval-rmse:0.284380\n",
      "[227]\ttrain-rmse:0.165879\teval-rmse:0.284346\n",
      "[228]\ttrain-rmse:0.165696\teval-rmse:0.284295\n",
      "[229]\ttrain-rmse:0.165433\teval-rmse:0.284247\n",
      "[230]\ttrain-rmse:0.165099\teval-rmse:0.284215\n",
      "[231]\ttrain-rmse:0.164766\teval-rmse:0.284171\n",
      "[232]\ttrain-rmse:0.164581\teval-rmse:0.284139\n",
      "[233]\ttrain-rmse:0.164384\teval-rmse:0.284095\n",
      "[234]\ttrain-rmse:0.164089\teval-rmse:0.284112\n",
      "[235]\ttrain-rmse:0.163827\teval-rmse:0.284108\n",
      "[236]\ttrain-rmse:0.163725\teval-rmse:0.284087\n",
      "[237]\ttrain-rmse:0.163373\teval-rmse:0.284014\n",
      "[238]\ttrain-rmse:0.163183\teval-rmse:0.283987\n",
      "[239]\ttrain-rmse:0.162966\teval-rmse:0.283970\n",
      "[240]\ttrain-rmse:0.162730\teval-rmse:0.283924\n",
      "[241]\ttrain-rmse:0.162493\teval-rmse:0.283902\n",
      "[242]\ttrain-rmse:0.162285\teval-rmse:0.283892\n",
      "[243]\ttrain-rmse:0.162054\teval-rmse:0.283874\n",
      "[244]\ttrain-rmse:0.161731\teval-rmse:0.283850\n",
      "[245]\ttrain-rmse:0.161537\teval-rmse:0.283823\n",
      "[246]\ttrain-rmse:0.161326\teval-rmse:0.283826\n",
      "[247]\ttrain-rmse:0.161157\teval-rmse:0.283828\n",
      "[248]\ttrain-rmse:0.160967\teval-rmse:0.283813\n",
      "[249]\ttrain-rmse:0.160814\teval-rmse:0.283802\n",
      "[250]\ttrain-rmse:0.160694\teval-rmse:0.283775\n",
      "[251]\ttrain-rmse:0.160526\teval-rmse:0.283738\n",
      "[252]\ttrain-rmse:0.160362\teval-rmse:0.283709\n",
      "[253]\ttrain-rmse:0.160211\teval-rmse:0.283716\n",
      "[254]\ttrain-rmse:0.159983\teval-rmse:0.283708\n",
      "[255]\ttrain-rmse:0.159828\teval-rmse:0.283700\n",
      "[256]\ttrain-rmse:0.159660\teval-rmse:0.283669\n",
      "[257]\ttrain-rmse:0.159511\teval-rmse:0.283641\n",
      "[258]\ttrain-rmse:0.159259\teval-rmse:0.283590\n",
      "[259]\ttrain-rmse:0.159088\teval-rmse:0.283599\n",
      "[260]\ttrain-rmse:0.158945\teval-rmse:0.283577\n",
      "[261]\ttrain-rmse:0.158856\teval-rmse:0.283561\n",
      "[262]\ttrain-rmse:0.158686\teval-rmse:0.283526\n",
      "[263]\ttrain-rmse:0.158527\teval-rmse:0.283498\n",
      "[264]\ttrain-rmse:0.158417\teval-rmse:0.283498\n",
      "[265]\ttrain-rmse:0.158213\teval-rmse:0.283482\n",
      "[266]\ttrain-rmse:0.158051\teval-rmse:0.283460\n",
      "[267]\ttrain-rmse:0.157949\teval-rmse:0.283448\n",
      "[268]\ttrain-rmse:0.157721\teval-rmse:0.283429\n",
      "[269]\ttrain-rmse:0.157451\teval-rmse:0.283420\n",
      "[270]\ttrain-rmse:0.157261\teval-rmse:0.283404\n",
      "[271]\ttrain-rmse:0.157088\teval-rmse:0.283389\n",
      "[272]\ttrain-rmse:0.156880\teval-rmse:0.283395\n",
      "[273]\ttrain-rmse:0.156741\teval-rmse:0.283383\n",
      "[274]\ttrain-rmse:0.156598\teval-rmse:0.283383\n",
      "[275]\ttrain-rmse:0.156427\teval-rmse:0.283400\n",
      "[276]\ttrain-rmse:0.156212\teval-rmse:0.283355\n",
      "[277]\ttrain-rmse:0.155991\teval-rmse:0.283326\n",
      "[278]\ttrain-rmse:0.155865\teval-rmse:0.283303\n",
      "[279]\ttrain-rmse:0.155617\teval-rmse:0.283264\n",
      "[280]\ttrain-rmse:0.155395\teval-rmse:0.283238\n",
      "[281]\ttrain-rmse:0.155297\teval-rmse:0.283204\n",
      "[282]\ttrain-rmse:0.155143\teval-rmse:0.283224\n",
      "[283]\ttrain-rmse:0.154929\teval-rmse:0.283224\n",
      "[284]\ttrain-rmse:0.154769\teval-rmse:0.283214\n",
      "[285]\ttrain-rmse:0.154610\teval-rmse:0.283190\n",
      "[286]\ttrain-rmse:0.154432\teval-rmse:0.283183\n",
      "[287]\ttrain-rmse:0.154185\teval-rmse:0.283129\n",
      "[288]\ttrain-rmse:0.153959\teval-rmse:0.283116\n",
      "[289]\ttrain-rmse:0.153799\teval-rmse:0.283109\n",
      "[290]\ttrain-rmse:0.153630\teval-rmse:0.283100\n",
      "[291]\ttrain-rmse:0.153476\teval-rmse:0.283094\n",
      "[292]\ttrain-rmse:0.153398\teval-rmse:0.283076\n",
      "[293]\ttrain-rmse:0.153323\teval-rmse:0.283066\n",
      "[294]\ttrain-rmse:0.153186\teval-rmse:0.283032\n",
      "[295]\ttrain-rmse:0.153086\teval-rmse:0.283014\n",
      "[296]\ttrain-rmse:0.152946\teval-rmse:0.282993\n",
      "[297]\ttrain-rmse:0.152787\teval-rmse:0.283007\n",
      "[298]\ttrain-rmse:0.152651\teval-rmse:0.282983\n",
      "[299]\ttrain-rmse:0.152455\teval-rmse:0.282976\n",
      "[300]\ttrain-rmse:0.152338\teval-rmse:0.282944\n",
      "[301]\ttrain-rmse:0.152157\teval-rmse:0.282915\n",
      "[302]\ttrain-rmse:0.152069\teval-rmse:0.282923\n",
      "[303]\ttrain-rmse:0.151844\teval-rmse:0.282895\n",
      "[304]\ttrain-rmse:0.151752\teval-rmse:0.282890\n",
      "[305]\ttrain-rmse:0.151539\teval-rmse:0.282868\n",
      "[306]\ttrain-rmse:0.151366\teval-rmse:0.282826\n",
      "[307]\ttrain-rmse:0.151223\teval-rmse:0.282826\n",
      "[308]\ttrain-rmse:0.150981\teval-rmse:0.282817\n",
      "[309]\ttrain-rmse:0.150658\teval-rmse:0.282784\n",
      "[310]\ttrain-rmse:0.150475\teval-rmse:0.282761\n",
      "[311]\ttrain-rmse:0.150342\teval-rmse:0.282768\n",
      "[312]\ttrain-rmse:0.150229\teval-rmse:0.282761\n",
      "[313]\ttrain-rmse:0.150135\teval-rmse:0.282724\n",
      "[314]\ttrain-rmse:0.149975\teval-rmse:0.282728\n",
      "[315]\ttrain-rmse:0.149838\teval-rmse:0.282702\n",
      "[316]\ttrain-rmse:0.149776\teval-rmse:0.282705\n",
      "[317]\ttrain-rmse:0.149729\teval-rmse:0.282688\n",
      "[318]\ttrain-rmse:0.149584\teval-rmse:0.282668\n",
      "[319]\ttrain-rmse:0.149431\teval-rmse:0.282669\n",
      "[320]\ttrain-rmse:0.149327\teval-rmse:0.282652\n",
      "[321]\ttrain-rmse:0.149264\teval-rmse:0.282666\n",
      "[322]\ttrain-rmse:0.149066\teval-rmse:0.282645\n",
      "[323]\ttrain-rmse:0.148881\teval-rmse:0.282638\n",
      "[324]\ttrain-rmse:0.148818\teval-rmse:0.282630\n",
      "[325]\ttrain-rmse:0.148780\teval-rmse:0.282632\n",
      "[326]\ttrain-rmse:0.148637\teval-rmse:0.282610\n",
      "[327]\ttrain-rmse:0.148474\teval-rmse:0.282609\n",
      "[328]\ttrain-rmse:0.148397\teval-rmse:0.282582\n",
      "[329]\ttrain-rmse:0.148269\teval-rmse:0.282564\n",
      "[330]\ttrain-rmse:0.148169\teval-rmse:0.282568\n",
      "[331]\ttrain-rmse:0.148012\teval-rmse:0.282574\n",
      "[332]\ttrain-rmse:0.147865\teval-rmse:0.282566\n",
      "[333]\ttrain-rmse:0.147785\teval-rmse:0.282535\n",
      "[334]\ttrain-rmse:0.147594\teval-rmse:0.282506\n",
      "[335]\ttrain-rmse:0.147379\teval-rmse:0.282494\n",
      "[336]\ttrain-rmse:0.147181\teval-rmse:0.282499\n",
      "[337]\ttrain-rmse:0.147139\teval-rmse:0.282505\n",
      "[338]\ttrain-rmse:0.147114\teval-rmse:0.282505\n",
      "[339]\ttrain-rmse:0.147033\teval-rmse:0.282491\n",
      "[340]\ttrain-rmse:0.146758\teval-rmse:0.282482\n",
      "[341]\ttrain-rmse:0.146642\teval-rmse:0.282488\n",
      "[342]\ttrain-rmse:0.146451\teval-rmse:0.282487\n",
      "[343]\ttrain-rmse:0.146378\teval-rmse:0.282492\n",
      "[344]\ttrain-rmse:0.146292\teval-rmse:0.282480\n",
      "[345]\ttrain-rmse:0.146180\teval-rmse:0.282462\n",
      "[346]\ttrain-rmse:0.146117\teval-rmse:0.282454\n",
      "[347]\ttrain-rmse:0.145988\teval-rmse:0.282446\n",
      "[348]\ttrain-rmse:0.145849\teval-rmse:0.282441\n",
      "[349]\ttrain-rmse:0.145593\teval-rmse:0.282419\n",
      "[350]\ttrain-rmse:0.145405\teval-rmse:0.282418\n",
      "[351]\ttrain-rmse:0.145225\teval-rmse:0.282381\n",
      "[352]\ttrain-rmse:0.145023\teval-rmse:0.282376\n",
      "[353]\ttrain-rmse:0.144830\teval-rmse:0.282347\n",
      "[354]\ttrain-rmse:0.144692\teval-rmse:0.282323\n",
      "[355]\ttrain-rmse:0.144545\teval-rmse:0.282333\n",
      "[356]\ttrain-rmse:0.144289\teval-rmse:0.282291\n",
      "[357]\ttrain-rmse:0.144145\teval-rmse:0.282264\n",
      "[358]\ttrain-rmse:0.143990\teval-rmse:0.282239\n",
      "[359]\ttrain-rmse:0.143822\teval-rmse:0.282224\n",
      "[360]\ttrain-rmse:0.143707\teval-rmse:0.282206\n",
      "[361]\ttrain-rmse:0.143662\teval-rmse:0.282187\n",
      "[362]\ttrain-rmse:0.143606\teval-rmse:0.282175\n",
      "[363]\ttrain-rmse:0.143497\teval-rmse:0.282170\n",
      "[364]\ttrain-rmse:0.143325\teval-rmse:0.282147\n",
      "[365]\ttrain-rmse:0.143159\teval-rmse:0.282110\n",
      "[366]\ttrain-rmse:0.143083\teval-rmse:0.282112\n",
      "[367]\ttrain-rmse:0.142906\teval-rmse:0.282078\n",
      "[368]\ttrain-rmse:0.142740\teval-rmse:0.282063\n",
      "[369]\ttrain-rmse:0.142584\teval-rmse:0.282085\n",
      "[370]\ttrain-rmse:0.142490\teval-rmse:0.282101\n",
      "[371]\ttrain-rmse:0.142325\teval-rmse:0.282095\n",
      "[372]\ttrain-rmse:0.142186\teval-rmse:0.282087\n",
      "[373]\ttrain-rmse:0.142121\teval-rmse:0.282062\n",
      "[374]\ttrain-rmse:0.142040\teval-rmse:0.282065\n",
      "[375]\ttrain-rmse:0.141889\teval-rmse:0.282032\n",
      "[376]\ttrain-rmse:0.141812\teval-rmse:0.282025\n",
      "[377]\ttrain-rmse:0.141791\teval-rmse:0.282025\n",
      "[378]\ttrain-rmse:0.141738\teval-rmse:0.282016\n",
      "[379]\ttrain-rmse:0.141613\teval-rmse:0.282002\n",
      "[380]\ttrain-rmse:0.141462\teval-rmse:0.281989\n",
      "[381]\ttrain-rmse:0.141336\teval-rmse:0.282006\n",
      "[382]\ttrain-rmse:0.141179\teval-rmse:0.281992\n",
      "[383]\ttrain-rmse:0.140940\teval-rmse:0.281945\n",
      "[384]\ttrain-rmse:0.140814\teval-rmse:0.281941\n",
      "[385]\ttrain-rmse:0.140675\teval-rmse:0.281929\n",
      "[386]\ttrain-rmse:0.140594\teval-rmse:0.281937\n",
      "[387]\ttrain-rmse:0.140497\teval-rmse:0.281941\n",
      "[388]\ttrain-rmse:0.140302\teval-rmse:0.281924\n",
      "[389]\ttrain-rmse:0.140214\teval-rmse:0.281910\n",
      "[390]\ttrain-rmse:0.140112\teval-rmse:0.281921\n",
      "[391]\ttrain-rmse:0.140016\teval-rmse:0.281909\n",
      "[392]\ttrain-rmse:0.139961\teval-rmse:0.281908\n",
      "[393]\ttrain-rmse:0.139780\teval-rmse:0.281923\n",
      "[394]\ttrain-rmse:0.139536\teval-rmse:0.281911\n",
      "[395]\ttrain-rmse:0.139449\teval-rmse:0.281910\n",
      "[396]\ttrain-rmse:0.139293\teval-rmse:0.281902\n",
      "[397]\ttrain-rmse:0.139139\teval-rmse:0.281888\n",
      "[398]\ttrain-rmse:0.138982\teval-rmse:0.281874\n",
      "[399]\ttrain-rmse:0.138896\teval-rmse:0.281860\n",
      "[400]\ttrain-rmse:0.138834\teval-rmse:0.281858\n",
      "[401]\ttrain-rmse:0.138780\teval-rmse:0.281844\n",
      "[402]\ttrain-rmse:0.138685\teval-rmse:0.281819\n",
      "[403]\ttrain-rmse:0.138626\teval-rmse:0.281824\n",
      "[404]\ttrain-rmse:0.138487\teval-rmse:0.281820\n",
      "[405]\ttrain-rmse:0.138402\teval-rmse:0.281836\n",
      "[406]\ttrain-rmse:0.138371\teval-rmse:0.281832\n",
      "[407]\ttrain-rmse:0.138342\teval-rmse:0.281823\n",
      "[408]\ttrain-rmse:0.138181\teval-rmse:0.281800\n",
      "[409]\ttrain-rmse:0.138040\teval-rmse:0.281784\n",
      "[410]\ttrain-rmse:0.138008\teval-rmse:0.281778\n",
      "[411]\ttrain-rmse:0.137815\teval-rmse:0.281776\n",
      "[412]\ttrain-rmse:0.137712\teval-rmse:0.281781\n",
      "[413]\ttrain-rmse:0.137572\teval-rmse:0.281790\n",
      "[414]\ttrain-rmse:0.137482\teval-rmse:0.281784\n",
      "[415]\ttrain-rmse:0.137402\teval-rmse:0.281788\n",
      "[416]\ttrain-rmse:0.137273\teval-rmse:0.281774\n",
      "[417]\ttrain-rmse:0.137226\teval-rmse:0.281773\n",
      "[418]\ttrain-rmse:0.137054\teval-rmse:0.281730\n",
      "[419]\ttrain-rmse:0.136967\teval-rmse:0.281712\n",
      "[420]\ttrain-rmse:0.136879\teval-rmse:0.281711\n",
      "[421]\ttrain-rmse:0.136757\teval-rmse:0.281708\n",
      "[422]\ttrain-rmse:0.136557\teval-rmse:0.281702\n",
      "[423]\ttrain-rmse:0.136433\teval-rmse:0.281698\n",
      "[424]\ttrain-rmse:0.136372\teval-rmse:0.281686\n",
      "[425]\ttrain-rmse:0.136246\teval-rmse:0.281666\n",
      "[426]\ttrain-rmse:0.136175\teval-rmse:0.281649\n",
      "[427]\ttrain-rmse:0.136023\teval-rmse:0.281629\n",
      "[428]\ttrain-rmse:0.135802\teval-rmse:0.281608\n",
      "[429]\ttrain-rmse:0.135695\teval-rmse:0.281599\n",
      "[430]\ttrain-rmse:0.135519\teval-rmse:0.281600\n",
      "[431]\ttrain-rmse:0.135493\teval-rmse:0.281590\n",
      "[432]\ttrain-rmse:0.135414\teval-rmse:0.281579\n",
      "[433]\ttrain-rmse:0.135249\teval-rmse:0.281559\n",
      "[434]\ttrain-rmse:0.135154\teval-rmse:0.281559\n",
      "[435]\ttrain-rmse:0.135067\teval-rmse:0.281545\n",
      "[436]\ttrain-rmse:0.134891\teval-rmse:0.281545\n",
      "[437]\ttrain-rmse:0.134810\teval-rmse:0.281554\n",
      "[438]\ttrain-rmse:0.134743\teval-rmse:0.281556\n",
      "[439]\ttrain-rmse:0.134614\teval-rmse:0.281538\n",
      "[440]\ttrain-rmse:0.134491\teval-rmse:0.281508\n",
      "[441]\ttrain-rmse:0.134380\teval-rmse:0.281491\n",
      "[442]\ttrain-rmse:0.134350\teval-rmse:0.281486\n",
      "[443]\ttrain-rmse:0.134179\teval-rmse:0.281495\n",
      "[444]\ttrain-rmse:0.134095\teval-rmse:0.281493\n",
      "[445]\ttrain-rmse:0.134008\teval-rmse:0.281487\n",
      "[446]\ttrain-rmse:0.133924\teval-rmse:0.281486\n",
      "[447]\ttrain-rmse:0.133851\teval-rmse:0.281473\n",
      "[448]\ttrain-rmse:0.133654\teval-rmse:0.281423\n",
      "[449]\ttrain-rmse:0.133473\teval-rmse:0.281448\n",
      "[450]\ttrain-rmse:0.133425\teval-rmse:0.281453\n",
      "[451]\ttrain-rmse:0.133216\teval-rmse:0.281421\n",
      "[452]\ttrain-rmse:0.133148\teval-rmse:0.281406\n",
      "[453]\ttrain-rmse:0.132905\teval-rmse:0.281373\n",
      "[454]\ttrain-rmse:0.132865\teval-rmse:0.281366\n",
      "[455]\ttrain-rmse:0.132826\teval-rmse:0.281374\n",
      "[456]\ttrain-rmse:0.132702\teval-rmse:0.281376\n",
      "[457]\ttrain-rmse:0.132472\teval-rmse:0.281373\n",
      "[458]\ttrain-rmse:0.132326\teval-rmse:0.281366\n",
      "[459]\ttrain-rmse:0.132202\teval-rmse:0.281352\n",
      "[460]\ttrain-rmse:0.132100\teval-rmse:0.281347\n",
      "[461]\ttrain-rmse:0.131936\teval-rmse:0.281335\n",
      "[462]\ttrain-rmse:0.131853\teval-rmse:0.281351\n",
      "[463]\ttrain-rmse:0.131731\teval-rmse:0.281357\n",
      "[464]\ttrain-rmse:0.131551\teval-rmse:0.281352\n",
      "[465]\ttrain-rmse:0.131340\teval-rmse:0.281352\n",
      "[466]\ttrain-rmse:0.131175\teval-rmse:0.281335\n",
      "[467]\ttrain-rmse:0.130981\teval-rmse:0.281327\n",
      "[468]\ttrain-rmse:0.130902\teval-rmse:0.281333\n",
      "[469]\ttrain-rmse:0.130796\teval-rmse:0.281350\n",
      "[470]\ttrain-rmse:0.130689\teval-rmse:0.281344\n",
      "[471]\ttrain-rmse:0.130565\teval-rmse:0.281344\n",
      "[472]\ttrain-rmse:0.130426\teval-rmse:0.281358\n",
      "[473]\ttrain-rmse:0.130302\teval-rmse:0.281328\n",
      "[474]\ttrain-rmse:0.130240\teval-rmse:0.281303\n",
      "[475]\ttrain-rmse:0.130166\teval-rmse:0.281300\n",
      "[476]\ttrain-rmse:0.130120\teval-rmse:0.281307\n",
      "[477]\ttrain-rmse:0.130069\teval-rmse:0.281297\n",
      "[478]\ttrain-rmse:0.129993\teval-rmse:0.281291\n",
      "[479]\ttrain-rmse:0.129815\teval-rmse:0.281276\n",
      "[480]\ttrain-rmse:0.129718\teval-rmse:0.281270\n",
      "[481]\ttrain-rmse:0.129650\teval-rmse:0.281255\n",
      "[482]\ttrain-rmse:0.129478\teval-rmse:0.281227\n",
      "[483]\ttrain-rmse:0.129208\teval-rmse:0.281219\n",
      "[484]\ttrain-rmse:0.129103\teval-rmse:0.281211\n",
      "[485]\ttrain-rmse:0.128974\teval-rmse:0.281209\n",
      "[486]\ttrain-rmse:0.128927\teval-rmse:0.281197\n",
      "[487]\ttrain-rmse:0.128755\teval-rmse:0.281179\n",
      "[488]\ttrain-rmse:0.128672\teval-rmse:0.281176\n",
      "[489]\ttrain-rmse:0.128449\teval-rmse:0.281177\n",
      "[490]\ttrain-rmse:0.128254\teval-rmse:0.281171\n",
      "[491]\ttrain-rmse:0.128162\teval-rmse:0.281159\n",
      "[492]\ttrain-rmse:0.128041\teval-rmse:0.281170\n",
      "[493]\ttrain-rmse:0.127866\teval-rmse:0.281185\n",
      "[494]\ttrain-rmse:0.127778\teval-rmse:0.281186\n",
      "[495]\ttrain-rmse:0.127662\teval-rmse:0.281174\n",
      "[496]\ttrain-rmse:0.127609\teval-rmse:0.281171\n",
      "[497]\ttrain-rmse:0.127468\teval-rmse:0.281179\n",
      "[498]\ttrain-rmse:0.127441\teval-rmse:0.281175\n",
      "[499]\ttrain-rmse:0.127263\teval-rmse:0.281165\n",
      "[500]\ttrain-rmse:0.127217\teval-rmse:0.281166\n",
      "[501]\ttrain-rmse:0.126974\teval-rmse:0.281156\n",
      "[502]\ttrain-rmse:0.126791\teval-rmse:0.281145\n",
      "[503]\ttrain-rmse:0.126731\teval-rmse:0.281156\n",
      "[504]\ttrain-rmse:0.126618\teval-rmse:0.281141\n",
      "[505]\ttrain-rmse:0.126414\teval-rmse:0.281131\n",
      "[506]\ttrain-rmse:0.126299\teval-rmse:0.281116\n",
      "[507]\ttrain-rmse:0.126190\teval-rmse:0.281118\n",
      "[508]\ttrain-rmse:0.126084\teval-rmse:0.281122\n",
      "[509]\ttrain-rmse:0.126012\teval-rmse:0.281118\n",
      "[510]\ttrain-rmse:0.125923\teval-rmse:0.281090\n",
      "[511]\ttrain-rmse:0.125893\teval-rmse:0.281089\n",
      "[512]\ttrain-rmse:0.125825\teval-rmse:0.281100\n",
      "[513]\ttrain-rmse:0.125768\teval-rmse:0.281091\n",
      "[514]\ttrain-rmse:0.125739\teval-rmse:0.281087\n",
      "[515]\ttrain-rmse:0.125623\teval-rmse:0.281066\n",
      "[516]\ttrain-rmse:0.125585\teval-rmse:0.281059\n",
      "[517]\ttrain-rmse:0.125523\teval-rmse:0.281065\n",
      "[518]\ttrain-rmse:0.125366\teval-rmse:0.281067\n",
      "[519]\ttrain-rmse:0.125239\teval-rmse:0.281069\n",
      "[520]\ttrain-rmse:0.125093\teval-rmse:0.281057\n",
      "[521]\ttrain-rmse:0.125036\teval-rmse:0.281050\n",
      "[522]\ttrain-rmse:0.124978\teval-rmse:0.281042\n",
      "[523]\ttrain-rmse:0.124943\teval-rmse:0.281032\n",
      "[524]\ttrain-rmse:0.124842\teval-rmse:0.281027\n",
      "[525]\ttrain-rmse:0.124710\teval-rmse:0.281019\n",
      "[526]\ttrain-rmse:0.124517\teval-rmse:0.281034\n",
      "[527]\ttrain-rmse:0.124498\teval-rmse:0.281034\n",
      "[528]\ttrain-rmse:0.124377\teval-rmse:0.281022\n",
      "[529]\ttrain-rmse:0.124328\teval-rmse:0.281023\n",
      "[530]\ttrain-rmse:0.124267\teval-rmse:0.281046\n",
      "[531]\ttrain-rmse:0.124091\teval-rmse:0.281049\n",
      "[532]\ttrain-rmse:0.123931\teval-rmse:0.281044\n",
      "[533]\ttrain-rmse:0.123748\teval-rmse:0.281038\n",
      "[534]\ttrain-rmse:0.123596\teval-rmse:0.281033\n",
      "[535]\ttrain-rmse:0.123416\teval-rmse:0.281012\n",
      "[536]\ttrain-rmse:0.123275\teval-rmse:0.281019\n",
      "[537]\ttrain-rmse:0.123150\teval-rmse:0.281014\n",
      "[538]\ttrain-rmse:0.123050\teval-rmse:0.281010\n",
      "[539]\ttrain-rmse:0.123022\teval-rmse:0.281029\n",
      "[540]\ttrain-rmse:0.122878\teval-rmse:0.281017\n",
      "[541]\ttrain-rmse:0.122718\teval-rmse:0.281005\n",
      "[542]\ttrain-rmse:0.122689\teval-rmse:0.281003\n",
      "[543]\ttrain-rmse:0.122579\teval-rmse:0.280994\n",
      "[544]\ttrain-rmse:0.122429\teval-rmse:0.280998\n",
      "[545]\ttrain-rmse:0.122368\teval-rmse:0.281011\n",
      "[546]\ttrain-rmse:0.122236\teval-rmse:0.281019\n",
      "[547]\ttrain-rmse:0.122114\teval-rmse:0.281016\n",
      "[548]\ttrain-rmse:0.122068\teval-rmse:0.281003\n",
      "[549]\ttrain-rmse:0.121973\teval-rmse:0.280997\n",
      "[550]\ttrain-rmse:0.121958\teval-rmse:0.280998\n",
      "[551]\ttrain-rmse:0.121823\teval-rmse:0.281003\n",
      "[552]\ttrain-rmse:0.121778\teval-rmse:0.281012\n",
      "[553]\ttrain-rmse:0.121740\teval-rmse:0.281004\n",
      "[554]\ttrain-rmse:0.121661\teval-rmse:0.281016\n",
      "[555]\ttrain-rmse:0.121633\teval-rmse:0.281013\n",
      "[556]\ttrain-rmse:0.121487\teval-rmse:0.281006\n",
      "[557]\ttrain-rmse:0.121373\teval-rmse:0.280997\n",
      "[558]\ttrain-rmse:0.121173\teval-rmse:0.280980\n",
      "[559]\ttrain-rmse:0.120981\teval-rmse:0.281003\n",
      "[560]\ttrain-rmse:0.120870\teval-rmse:0.280995\n",
      "[561]\ttrain-rmse:0.120798\teval-rmse:0.280997\n",
      "[562]\ttrain-rmse:0.120707\teval-rmse:0.280993\n",
      "[563]\ttrain-rmse:0.120556\teval-rmse:0.280973\n",
      "[564]\ttrain-rmse:0.120446\teval-rmse:0.280979\n",
      "[565]\ttrain-rmse:0.120298\teval-rmse:0.280968\n",
      "[566]\ttrain-rmse:0.120100\teval-rmse:0.280958\n",
      "[567]\ttrain-rmse:0.119984\teval-rmse:0.280951\n",
      "[568]\ttrain-rmse:0.119766\teval-rmse:0.280964\n",
      "[569]\ttrain-rmse:0.119575\teval-rmse:0.280944\n",
      "[570]\ttrain-rmse:0.119459\teval-rmse:0.280952\n",
      "[571]\ttrain-rmse:0.119332\teval-rmse:0.280937\n",
      "[572]\ttrain-rmse:0.119130\teval-rmse:0.280935\n",
      "[573]\ttrain-rmse:0.119102\teval-rmse:0.280931\n",
      "[574]\ttrain-rmse:0.119050\teval-rmse:0.280928\n",
      "[575]\ttrain-rmse:0.118905\teval-rmse:0.280944\n",
      "[576]\ttrain-rmse:0.118772\teval-rmse:0.280963\n",
      "[577]\ttrain-rmse:0.118585\teval-rmse:0.280945\n",
      "[578]\ttrain-rmse:0.118542\teval-rmse:0.280943\n",
      "[579]\ttrain-rmse:0.118390\teval-rmse:0.280948\n",
      "[580]\ttrain-rmse:0.118337\teval-rmse:0.280941\n",
      "[581]\ttrain-rmse:0.118309\teval-rmse:0.280930\n",
      "[582]\ttrain-rmse:0.118289\teval-rmse:0.280926\n",
      "[583]\ttrain-rmse:0.118126\teval-rmse:0.280928\n",
      "[584]\ttrain-rmse:0.118038\teval-rmse:0.280919\n",
      "[585]\ttrain-rmse:0.117954\teval-rmse:0.280929\n",
      "[586]\ttrain-rmse:0.117894\teval-rmse:0.280921\n",
      "[587]\ttrain-rmse:0.117797\teval-rmse:0.280886\n",
      "[588]\ttrain-rmse:0.117701\teval-rmse:0.280862\n",
      "[589]\ttrain-rmse:0.117673\teval-rmse:0.280860\n",
      "[590]\ttrain-rmse:0.117619\teval-rmse:0.280857\n",
      "[591]\ttrain-rmse:0.117456\teval-rmse:0.280850\n",
      "[592]\ttrain-rmse:0.117378\teval-rmse:0.280849\n",
      "[593]\ttrain-rmse:0.117331\teval-rmse:0.280836\n",
      "[594]\ttrain-rmse:0.117207\teval-rmse:0.280843\n",
      "[595]\ttrain-rmse:0.117158\teval-rmse:0.280840\n",
      "[596]\ttrain-rmse:0.117093\teval-rmse:0.280840\n",
      "[597]\ttrain-rmse:0.117037\teval-rmse:0.280821\n",
      "[598]\ttrain-rmse:0.116847\teval-rmse:0.280781\n",
      "[599]\ttrain-rmse:0.116730\teval-rmse:0.280781\n",
      "[600]\ttrain-rmse:0.116678\teval-rmse:0.280776\n",
      "[601]\ttrain-rmse:0.116571\teval-rmse:0.280806\n",
      "[602]\ttrain-rmse:0.116455\teval-rmse:0.280806\n",
      "[603]\ttrain-rmse:0.116370\teval-rmse:0.280794\n",
      "[604]\ttrain-rmse:0.116236\teval-rmse:0.280799\n",
      "[605]\ttrain-rmse:0.116114\teval-rmse:0.280773\n",
      "[606]\ttrain-rmse:0.116051\teval-rmse:0.280770\n",
      "[607]\ttrain-rmse:0.115964\teval-rmse:0.280763\n",
      "[608]\ttrain-rmse:0.115779\teval-rmse:0.280759\n",
      "[609]\ttrain-rmse:0.115711\teval-rmse:0.280764\n",
      "[610]\ttrain-rmse:0.115658\teval-rmse:0.280767\n",
      "[611]\ttrain-rmse:0.115635\teval-rmse:0.280765\n",
      "[612]\ttrain-rmse:0.115598\teval-rmse:0.280770\n",
      "[613]\ttrain-rmse:0.115539\teval-rmse:0.280776\n",
      "[614]\ttrain-rmse:0.115502\teval-rmse:0.280789\n",
      "[615]\ttrain-rmse:0.115381\teval-rmse:0.280796\n",
      "[616]\ttrain-rmse:0.115344\teval-rmse:0.280795\n",
      "[617]\ttrain-rmse:0.115234\teval-rmse:0.280795\n",
      "[618]\ttrain-rmse:0.115202\teval-rmse:0.280800\n",
      "[619]\ttrain-rmse:0.115060\teval-rmse:0.280770\n",
      "[620]\ttrain-rmse:0.114945\teval-rmse:0.280762\n",
      "[621]\ttrain-rmse:0.114867\teval-rmse:0.280751\n",
      "[622]\ttrain-rmse:0.114808\teval-rmse:0.280737\n",
      "[623]\ttrain-rmse:0.114732\teval-rmse:0.280733\n",
      "[624]\ttrain-rmse:0.114585\teval-rmse:0.280726\n",
      "[625]\ttrain-rmse:0.114554\teval-rmse:0.280718\n",
      "[626]\ttrain-rmse:0.114353\teval-rmse:0.280715\n",
      "[627]\ttrain-rmse:0.114156\teval-rmse:0.280705\n",
      "[628]\ttrain-rmse:0.114108\teval-rmse:0.280697\n",
      "[629]\ttrain-rmse:0.114018\teval-rmse:0.280697\n",
      "[630]\ttrain-rmse:0.113937\teval-rmse:0.280688\n",
      "[631]\ttrain-rmse:0.113796\teval-rmse:0.280675\n",
      "[632]\ttrain-rmse:0.113705\teval-rmse:0.280670\n",
      "[633]\ttrain-rmse:0.113609\teval-rmse:0.280673\n",
      "[634]\ttrain-rmse:0.113463\teval-rmse:0.280700\n",
      "[635]\ttrain-rmse:0.113367\teval-rmse:0.280703\n",
      "[636]\ttrain-rmse:0.113256\teval-rmse:0.280704\n",
      "[637]\ttrain-rmse:0.113087\teval-rmse:0.280692\n",
      "[638]\ttrain-rmse:0.112976\teval-rmse:0.280695\n",
      "[639]\ttrain-rmse:0.112929\teval-rmse:0.280702\n",
      "[640]\ttrain-rmse:0.112803\teval-rmse:0.280701\n",
      "[641]\ttrain-rmse:0.112628\teval-rmse:0.280684\n",
      "[642]\ttrain-rmse:0.112596\teval-rmse:0.280691\n",
      "[643]\ttrain-rmse:0.112432\teval-rmse:0.280700\n",
      "[644]\ttrain-rmse:0.112265\teval-rmse:0.280690\n",
      "[645]\ttrain-rmse:0.112179\teval-rmse:0.280687\n",
      "[646]\ttrain-rmse:0.112114\teval-rmse:0.280685\n",
      "[647]\ttrain-rmse:0.112011\teval-rmse:0.280706\n",
      "[648]\ttrain-rmse:0.111910\teval-rmse:0.280718\n",
      "[649]\ttrain-rmse:0.111759\teval-rmse:0.280733\n",
      "[650]\ttrain-rmse:0.111698\teval-rmse:0.280733\n",
      "[651]\ttrain-rmse:0.111554\teval-rmse:0.280708\n",
      "[652]\ttrain-rmse:0.111536\teval-rmse:0.280710\n",
      "[653]\ttrain-rmse:0.111450\teval-rmse:0.280710\n",
      "[654]\ttrain-rmse:0.111374\teval-rmse:0.280700\n",
      "[655]\ttrain-rmse:0.111261\teval-rmse:0.280714\n",
      "[656]\ttrain-rmse:0.111165\teval-rmse:0.280709\n",
      "[657]\ttrain-rmse:0.111088\teval-rmse:0.280690\n",
      "[658]\ttrain-rmse:0.111022\teval-rmse:0.280697\n",
      "[659]\ttrain-rmse:0.110916\teval-rmse:0.280698\n",
      "[660]\ttrain-rmse:0.110737\teval-rmse:0.280677\n",
      "[661]\ttrain-rmse:0.110684\teval-rmse:0.280673\n",
      "[662]\ttrain-rmse:0.110512\teval-rmse:0.280661\n",
      "[663]\ttrain-rmse:0.110447\teval-rmse:0.280668\n",
      "[664]\ttrain-rmse:0.110281\teval-rmse:0.280672\n",
      "[665]\ttrain-rmse:0.110226\teval-rmse:0.280667\n",
      "[666]\ttrain-rmse:0.110181\teval-rmse:0.280651\n",
      "[667]\ttrain-rmse:0.109973\teval-rmse:0.280644\n",
      "[668]\ttrain-rmse:0.109868\teval-rmse:0.280630\n",
      "[669]\ttrain-rmse:0.109805\teval-rmse:0.280625\n",
      "[670]\ttrain-rmse:0.109755\teval-rmse:0.280630\n",
      "[671]\ttrain-rmse:0.109640\teval-rmse:0.280622\n",
      "[672]\ttrain-rmse:0.109523\teval-rmse:0.280602\n",
      "[673]\ttrain-rmse:0.109382\teval-rmse:0.280600\n",
      "[674]\ttrain-rmse:0.109335\teval-rmse:0.280595\n",
      "[675]\ttrain-rmse:0.109298\teval-rmse:0.280593\n",
      "[676]\ttrain-rmse:0.109166\teval-rmse:0.280582\n",
      "[677]\ttrain-rmse:0.109031\teval-rmse:0.280565\n",
      "[678]\ttrain-rmse:0.108927\teval-rmse:0.280561\n",
      "[679]\ttrain-rmse:0.108836\teval-rmse:0.280566\n",
      "[680]\ttrain-rmse:0.108772\teval-rmse:0.280562\n",
      "[681]\ttrain-rmse:0.108707\teval-rmse:0.280565\n",
      "[682]\ttrain-rmse:0.108665\teval-rmse:0.280575\n",
      "[683]\ttrain-rmse:0.108606\teval-rmse:0.280581\n",
      "[684]\ttrain-rmse:0.108547\teval-rmse:0.280581\n",
      "[685]\ttrain-rmse:0.108523\teval-rmse:0.280583\n",
      "[686]\ttrain-rmse:0.108490\teval-rmse:0.280583\n",
      "[687]\ttrain-rmse:0.108341\teval-rmse:0.280578\n",
      "[688]\ttrain-rmse:0.108220\teval-rmse:0.280579\n",
      "[689]\ttrain-rmse:0.108091\teval-rmse:0.280556\n",
      "[690]\ttrain-rmse:0.108073\teval-rmse:0.280555\n",
      "[691]\ttrain-rmse:0.107940\teval-rmse:0.280570\n",
      "[692]\ttrain-rmse:0.107757\teval-rmse:0.280566\n",
      "[693]\ttrain-rmse:0.107558\teval-rmse:0.280545\n",
      "[694]\ttrain-rmse:0.107436\teval-rmse:0.280545\n",
      "[695]\ttrain-rmse:0.107264\teval-rmse:0.280536\n",
      "[696]\ttrain-rmse:0.107136\teval-rmse:0.280551\n",
      "[697]\ttrain-rmse:0.107030\teval-rmse:0.280538\n",
      "[698]\ttrain-rmse:0.106901\teval-rmse:0.280541\n",
      "[699]\ttrain-rmse:0.106860\teval-rmse:0.280535\n",
      "[700]\ttrain-rmse:0.106829\teval-rmse:0.280531\n",
      "[701]\ttrain-rmse:0.106769\teval-rmse:0.280547\n",
      "[702]\ttrain-rmse:0.106680\teval-rmse:0.280554\n",
      "[703]\ttrain-rmse:0.106524\teval-rmse:0.280554\n",
      "[704]\ttrain-rmse:0.106466\teval-rmse:0.280549\n",
      "[705]\ttrain-rmse:0.106427\teval-rmse:0.280549\n",
      "[706]\ttrain-rmse:0.106298\teval-rmse:0.280537\n",
      "[707]\ttrain-rmse:0.106197\teval-rmse:0.280524\n",
      "[708]\ttrain-rmse:0.106151\teval-rmse:0.280523\n",
      "[709]\ttrain-rmse:0.106061\teval-rmse:0.280533\n",
      "[710]\ttrain-rmse:0.106035\teval-rmse:0.280525\n",
      "[711]\ttrain-rmse:0.105860\teval-rmse:0.280527\n",
      "[712]\ttrain-rmse:0.105706\teval-rmse:0.280523\n",
      "[713]\ttrain-rmse:0.105607\teval-rmse:0.280520\n",
      "[714]\ttrain-rmse:0.105549\teval-rmse:0.280523\n",
      "[715]\ttrain-rmse:0.105506\teval-rmse:0.280515\n",
      "[716]\ttrain-rmse:0.105444\teval-rmse:0.280523\n",
      "[717]\ttrain-rmse:0.105406\teval-rmse:0.280526\n",
      "[718]\ttrain-rmse:0.105294\teval-rmse:0.280517\n",
      "[719]\ttrain-rmse:0.105066\teval-rmse:0.280516\n",
      "[720]\ttrain-rmse:0.104989\teval-rmse:0.280512\n",
      "[721]\ttrain-rmse:0.104906\teval-rmse:0.280517\n",
      "[722]\ttrain-rmse:0.104817\teval-rmse:0.280512\n",
      "[723]\ttrain-rmse:0.104737\teval-rmse:0.280493\n",
      "[724]\ttrain-rmse:0.104674\teval-rmse:0.280497\n",
      "[725]\ttrain-rmse:0.104569\teval-rmse:0.280483\n",
      "[726]\ttrain-rmse:0.104391\teval-rmse:0.280468\n",
      "[727]\ttrain-rmse:0.104355\teval-rmse:0.280470\n",
      "[728]\ttrain-rmse:0.104227\teval-rmse:0.280456\n",
      "[729]\ttrain-rmse:0.104151\teval-rmse:0.280469\n",
      "[730]\ttrain-rmse:0.104074\teval-rmse:0.280478\n",
      "[731]\ttrain-rmse:0.104006\teval-rmse:0.280483\n",
      "[732]\ttrain-rmse:0.103959\teval-rmse:0.280468\n",
      "[733]\ttrain-rmse:0.103822\teval-rmse:0.280467\n",
      "[734]\ttrain-rmse:0.103804\teval-rmse:0.280468\n",
      "[735]\ttrain-rmse:0.103714\teval-rmse:0.280459\n",
      "[736]\ttrain-rmse:0.103613\teval-rmse:0.280451\n",
      "[737]\ttrain-rmse:0.103567\teval-rmse:0.280454\n",
      "[738]\ttrain-rmse:0.103528\teval-rmse:0.280452\n",
      "[739]\ttrain-rmse:0.103444\teval-rmse:0.280436\n",
      "[740]\ttrain-rmse:0.103416\teval-rmse:0.280443\n",
      "[741]\ttrain-rmse:0.103226\teval-rmse:0.280436\n",
      "[742]\ttrain-rmse:0.103202\teval-rmse:0.280435\n",
      "[743]\ttrain-rmse:0.103083\teval-rmse:0.280440\n",
      "[744]\ttrain-rmse:0.103005\teval-rmse:0.280439\n",
      "[745]\ttrain-rmse:0.102979\teval-rmse:0.280441\n",
      "[746]\ttrain-rmse:0.102904\teval-rmse:0.280442\n",
      "[747]\ttrain-rmse:0.102872\teval-rmse:0.280435\n",
      "[748]\ttrain-rmse:0.102775\teval-rmse:0.280441\n",
      "[749]\ttrain-rmse:0.102709\teval-rmse:0.280438\n",
      "[750]\ttrain-rmse:0.102615\teval-rmse:0.280446\n",
      "[751]\ttrain-rmse:0.102520\teval-rmse:0.280448\n",
      "[752]\ttrain-rmse:0.102446\teval-rmse:0.280446\n",
      "[753]\ttrain-rmse:0.102413\teval-rmse:0.280447\n",
      "[754]\ttrain-rmse:0.102369\teval-rmse:0.280447\n",
      "[755]\ttrain-rmse:0.102300\teval-rmse:0.280442\n",
      "[756]\ttrain-rmse:0.102230\teval-rmse:0.280437\n",
      "[757]\ttrain-rmse:0.102005\teval-rmse:0.280437\n",
      "[758]\ttrain-rmse:0.101919\teval-rmse:0.280437\n",
      "[759]\ttrain-rmse:0.101835\teval-rmse:0.280416\n",
      "[760]\ttrain-rmse:0.101772\teval-rmse:0.280416\n",
      "[761]\ttrain-rmse:0.101669\teval-rmse:0.280417\n",
      "[762]\ttrain-rmse:0.101567\teval-rmse:0.280412\n",
      "[763]\ttrain-rmse:0.101440\teval-rmse:0.280427\n",
      "[764]\ttrain-rmse:0.101376\teval-rmse:0.280447\n",
      "[765]\ttrain-rmse:0.101224\teval-rmse:0.280446\n",
      "[766]\ttrain-rmse:0.101070\teval-rmse:0.280423\n",
      "[767]\ttrain-rmse:0.100962\teval-rmse:0.280421\n",
      "[768]\ttrain-rmse:0.100914\teval-rmse:0.280422\n",
      "[769]\ttrain-rmse:0.100833\teval-rmse:0.280412\n",
      "[770]\ttrain-rmse:0.100767\teval-rmse:0.280409\n",
      "[771]\ttrain-rmse:0.100678\teval-rmse:0.280426\n",
      "[772]\ttrain-rmse:0.100563\teval-rmse:0.280433\n",
      "[773]\ttrain-rmse:0.100424\teval-rmse:0.280444\n",
      "[774]\ttrain-rmse:0.100315\teval-rmse:0.280441\n",
      "[775]\ttrain-rmse:0.100248\teval-rmse:0.280438\n",
      "[776]\ttrain-rmse:0.100228\teval-rmse:0.280433\n",
      "[777]\ttrain-rmse:0.100131\teval-rmse:0.280414\n",
      "[778]\ttrain-rmse:0.100011\teval-rmse:0.280413\n",
      "[779]\ttrain-rmse:0.099972\teval-rmse:0.280413\n",
      "[780]\ttrain-rmse:0.099915\teval-rmse:0.280410\n",
      "[781]\ttrain-rmse:0.099824\teval-rmse:0.280412\n",
      "[782]\ttrain-rmse:0.099736\teval-rmse:0.280407\n",
      "[783]\ttrain-rmse:0.099673\teval-rmse:0.280398\n",
      "[784]\ttrain-rmse:0.099533\teval-rmse:0.280402\n",
      "[785]\ttrain-rmse:0.099481\teval-rmse:0.280395\n",
      "[786]\ttrain-rmse:0.099370\teval-rmse:0.280409\n",
      "[787]\ttrain-rmse:0.099341\teval-rmse:0.280411\n",
      "[788]\ttrain-rmse:0.099211\teval-rmse:0.280400\n",
      "[789]\ttrain-rmse:0.099071\teval-rmse:0.280419\n",
      "[790]\ttrain-rmse:0.098953\teval-rmse:0.280429\n",
      "[791]\ttrain-rmse:0.098923\teval-rmse:0.280430\n",
      "[792]\ttrain-rmse:0.098886\teval-rmse:0.280419\n",
      "[793]\ttrain-rmse:0.098730\teval-rmse:0.280391\n",
      "[794]\ttrain-rmse:0.098672\teval-rmse:0.280387\n",
      "[795]\ttrain-rmse:0.098615\teval-rmse:0.280396\n",
      "[796]\ttrain-rmse:0.098514\teval-rmse:0.280396\n",
      "[797]\ttrain-rmse:0.098422\teval-rmse:0.280382\n",
      "[798]\ttrain-rmse:0.098368\teval-rmse:0.280373\n",
      "[799]\ttrain-rmse:0.098346\teval-rmse:0.280375\n",
      "[800]\ttrain-rmse:0.098323\teval-rmse:0.280375\n",
      "[801]\ttrain-rmse:0.098121\teval-rmse:0.280342\n",
      "[802]\ttrain-rmse:0.098072\teval-rmse:0.280344\n",
      "[803]\ttrain-rmse:0.098006\teval-rmse:0.280347\n",
      "[804]\ttrain-rmse:0.097946\teval-rmse:0.280353\n",
      "[805]\ttrain-rmse:0.097838\teval-rmse:0.280357\n",
      "[806]\ttrain-rmse:0.097808\teval-rmse:0.280362\n",
      "[807]\ttrain-rmse:0.097769\teval-rmse:0.280360\n",
      "[808]\ttrain-rmse:0.097666\teval-rmse:0.280351\n",
      "[809]\ttrain-rmse:0.097493\teval-rmse:0.280355\n",
      "[810]\ttrain-rmse:0.097392\teval-rmse:0.280348\n",
      "[811]\ttrain-rmse:0.097308\teval-rmse:0.280353\n",
      "[812]\ttrain-rmse:0.097284\teval-rmse:0.280357\n",
      "[813]\ttrain-rmse:0.097173\teval-rmse:0.280353\n",
      "[814]\ttrain-rmse:0.097121\teval-rmse:0.280359\n",
      "[815]\ttrain-rmse:0.097037\teval-rmse:0.280352\n",
      "[816]\ttrain-rmse:0.096928\teval-rmse:0.280344\n",
      "[817]\ttrain-rmse:0.096874\teval-rmse:0.280357\n",
      "[818]\ttrain-rmse:0.096790\teval-rmse:0.280340\n",
      "[819]\ttrain-rmse:0.096694\teval-rmse:0.280342\n",
      "[820]\ttrain-rmse:0.096596\teval-rmse:0.280322\n",
      "[821]\ttrain-rmse:0.096514\teval-rmse:0.280320\n",
      "[822]\ttrain-rmse:0.096407\teval-rmse:0.280337\n",
      "[823]\ttrain-rmse:0.096353\teval-rmse:0.280338\n",
      "[824]\ttrain-rmse:0.096327\teval-rmse:0.280342\n",
      "[825]\ttrain-rmse:0.096280\teval-rmse:0.280351\n",
      "[826]\ttrain-rmse:0.096234\teval-rmse:0.280352\n",
      "[827]\ttrain-rmse:0.096131\teval-rmse:0.280340\n",
      "[828]\ttrain-rmse:0.096084\teval-rmse:0.280337\n",
      "[829]\ttrain-rmse:0.096040\teval-rmse:0.280333\n",
      "[830]\ttrain-rmse:0.096020\teval-rmse:0.280337\n",
      "[831]\ttrain-rmse:0.095884\teval-rmse:0.280328\n",
      "[832]\ttrain-rmse:0.095778\teval-rmse:0.280337\n",
      "[833]\ttrain-rmse:0.095719\teval-rmse:0.280337\n",
      "[834]\ttrain-rmse:0.095621\teval-rmse:0.280331\n",
      "[835]\ttrain-rmse:0.095541\teval-rmse:0.280322\n",
      "[836]\ttrain-rmse:0.095499\teval-rmse:0.280322\n",
      "[837]\ttrain-rmse:0.095482\teval-rmse:0.280323\n",
      "[838]\ttrain-rmse:0.095401\teval-rmse:0.280314\n",
      "[839]\ttrain-rmse:0.095301\teval-rmse:0.280328\n",
      "[840]\ttrain-rmse:0.095269\teval-rmse:0.280328\n",
      "[841]\ttrain-rmse:0.095139\teval-rmse:0.280318\n",
      "[842]\ttrain-rmse:0.095068\teval-rmse:0.280313\n",
      "[843]\ttrain-rmse:0.095038\teval-rmse:0.280307\n",
      "[844]\ttrain-rmse:0.094949\teval-rmse:0.280309\n",
      "[845]\ttrain-rmse:0.094877\teval-rmse:0.280319\n",
      "[846]\ttrain-rmse:0.094828\teval-rmse:0.280307\n",
      "[847]\ttrain-rmse:0.094705\teval-rmse:0.280301\n",
      "[848]\ttrain-rmse:0.094660\teval-rmse:0.280301\n",
      "[849]\ttrain-rmse:0.094624\teval-rmse:0.280294\n",
      "[850]\ttrain-rmse:0.094577\teval-rmse:0.280292\n",
      "[851]\ttrain-rmse:0.094442\teval-rmse:0.280304\n",
      "[852]\ttrain-rmse:0.094359\teval-rmse:0.280302\n",
      "[853]\ttrain-rmse:0.094335\teval-rmse:0.280305\n",
      "[854]\ttrain-rmse:0.094256\teval-rmse:0.280299\n",
      "[855]\ttrain-rmse:0.094189\teval-rmse:0.280293\n",
      "[856]\ttrain-rmse:0.094172\teval-rmse:0.280288\n",
      "[857]\ttrain-rmse:0.094112\teval-rmse:0.280293\n",
      "[858]\ttrain-rmse:0.094041\teval-rmse:0.280292\n",
      "[859]\ttrain-rmse:0.093944\teval-rmse:0.280300\n",
      "[860]\ttrain-rmse:0.093876\teval-rmse:0.280300\n",
      "[861]\ttrain-rmse:0.093777\teval-rmse:0.280294\n",
      "[862]\ttrain-rmse:0.093723\teval-rmse:0.280291\n",
      "[863]\ttrain-rmse:0.093629\teval-rmse:0.280297\n",
      "[864]\ttrain-rmse:0.093573\teval-rmse:0.280295\n",
      "[865]\ttrain-rmse:0.093547\teval-rmse:0.280297\n",
      "[866]\ttrain-rmse:0.093452\teval-rmse:0.280282\n",
      "[867]\ttrain-rmse:0.093423\teval-rmse:0.280289\n",
      "[868]\ttrain-rmse:0.093398\teval-rmse:0.280286\n",
      "[869]\ttrain-rmse:0.093326\teval-rmse:0.280275\n",
      "[870]\ttrain-rmse:0.093211\teval-rmse:0.280264\n",
      "[871]\ttrain-rmse:0.093117\teval-rmse:0.280266\n",
      "[872]\ttrain-rmse:0.093054\teval-rmse:0.280266\n",
      "[873]\ttrain-rmse:0.092959\teval-rmse:0.280256\n",
      "[874]\ttrain-rmse:0.092831\teval-rmse:0.280245\n",
      "[875]\ttrain-rmse:0.092744\teval-rmse:0.280230\n",
      "[876]\ttrain-rmse:0.092637\teval-rmse:0.280244\n",
      "[877]\ttrain-rmse:0.092534\teval-rmse:0.280241\n",
      "[878]\ttrain-rmse:0.092418\teval-rmse:0.280227\n",
      "[879]\ttrain-rmse:0.092356\teval-rmse:0.280224\n",
      "[880]\ttrain-rmse:0.092261\teval-rmse:0.280227\n",
      "[881]\ttrain-rmse:0.092157\teval-rmse:0.280231\n",
      "[882]\ttrain-rmse:0.092102\teval-rmse:0.280231\n",
      "[883]\ttrain-rmse:0.092010\teval-rmse:0.280238\n",
      "[884]\ttrain-rmse:0.091893\teval-rmse:0.280239\n",
      "[885]\ttrain-rmse:0.091862\teval-rmse:0.280237\n",
      "[886]\ttrain-rmse:0.091737\teval-rmse:0.280253\n",
      "[887]\ttrain-rmse:0.091648\teval-rmse:0.280266\n",
      "[888]\ttrain-rmse:0.091493\teval-rmse:0.280268\n",
      "[889]\ttrain-rmse:0.091430\teval-rmse:0.280256\n",
      "[890]\ttrain-rmse:0.091337\teval-rmse:0.280256\n",
      "[891]\ttrain-rmse:0.091243\teval-rmse:0.280265\n",
      "[892]\ttrain-rmse:0.091130\teval-rmse:0.280277\n",
      "[893]\ttrain-rmse:0.091037\teval-rmse:0.280265\n",
      "[894]\ttrain-rmse:0.090950\teval-rmse:0.280273\n",
      "[895]\ttrain-rmse:0.090826\teval-rmse:0.280274\n",
      "[896]\ttrain-rmse:0.090748\teval-rmse:0.280282\n",
      "[897]\ttrain-rmse:0.090734\teval-rmse:0.280281\n",
      "[898]\ttrain-rmse:0.090665\teval-rmse:0.280271\n",
      "[899]\ttrain-rmse:0.090613\teval-rmse:0.280276\n",
      "[900]\ttrain-rmse:0.090514\teval-rmse:0.280272\n",
      "[901]\ttrain-rmse:0.090398\teval-rmse:0.280263\n",
      "[902]\ttrain-rmse:0.090313\teval-rmse:0.280261\n",
      "[903]\ttrain-rmse:0.090220\teval-rmse:0.280255\n",
      "[904]\ttrain-rmse:0.090158\teval-rmse:0.280251\n",
      "[905]\ttrain-rmse:0.090104\teval-rmse:0.280246\n",
      "[906]\ttrain-rmse:0.090068\teval-rmse:0.280252\n",
      "[907]\ttrain-rmse:0.089992\teval-rmse:0.280257\n",
      "[908]\ttrain-rmse:0.089895\teval-rmse:0.280256\n",
      "[909]\ttrain-rmse:0.089860\teval-rmse:0.280262\n",
      "[910]\ttrain-rmse:0.089825\teval-rmse:0.280253\n",
      "[911]\ttrain-rmse:0.089758\teval-rmse:0.280248\n",
      "[912]\ttrain-rmse:0.089664\teval-rmse:0.280250\n",
      "[913]\ttrain-rmse:0.089548\teval-rmse:0.280248\n",
      "[914]\ttrain-rmse:0.089422\teval-rmse:0.280247\n",
      "[915]\ttrain-rmse:0.089372\teval-rmse:0.280240\n",
      "[916]\ttrain-rmse:0.089238\teval-rmse:0.280247\n",
      "[917]\ttrain-rmse:0.089185\teval-rmse:0.280247\n",
      "[918]\ttrain-rmse:0.089150\teval-rmse:0.280247\n",
      "[919]\ttrain-rmse:0.089063\teval-rmse:0.280248\n",
      "[920]\ttrain-rmse:0.088959\teval-rmse:0.280259\n",
      "[921]\ttrain-rmse:0.088941\teval-rmse:0.280259\n",
      "[922]\ttrain-rmse:0.088863\teval-rmse:0.280270\n",
      "[923]\ttrain-rmse:0.088841\teval-rmse:0.280269\n",
      "[924]\ttrain-rmse:0.088775\teval-rmse:0.280270\n",
      "[925]\ttrain-rmse:0.088735\teval-rmse:0.280268\n",
      "[926]\ttrain-rmse:0.088689\teval-rmse:0.280263\n",
      "[927]\ttrain-rmse:0.088608\teval-rmse:0.280266\n",
      "[928]\ttrain-rmse:0.088532\teval-rmse:0.280252\n",
      "[929]\ttrain-rmse:0.088446\teval-rmse:0.280264\n",
      "[930]\ttrain-rmse:0.088337\teval-rmse:0.280273\n",
      "[931]\ttrain-rmse:0.088280\teval-rmse:0.280257\n",
      "[932]\ttrain-rmse:0.088178\teval-rmse:0.280276\n",
      "[933]\ttrain-rmse:0.088152\teval-rmse:0.280280\n",
      "[934]\ttrain-rmse:0.088101\teval-rmse:0.280285\n",
      "[935]\ttrain-rmse:0.087978\teval-rmse:0.280301\n",
      "[936]\ttrain-rmse:0.087903\teval-rmse:0.280318\n",
      "[937]\ttrain-rmse:0.087793\teval-rmse:0.280319\n",
      "[938]\ttrain-rmse:0.087715\teval-rmse:0.280323\n",
      "[939]\ttrain-rmse:0.087566\teval-rmse:0.280325\n",
      "[940]\ttrain-rmse:0.087526\teval-rmse:0.280324\n",
      "[941]\ttrain-rmse:0.087415\teval-rmse:0.280335\n",
      "[942]\ttrain-rmse:0.087342\teval-rmse:0.280332\n",
      "[943]\ttrain-rmse:0.087277\teval-rmse:0.280326\n",
      "[944]\ttrain-rmse:0.087187\teval-rmse:0.280331\n",
      "[945]\ttrain-rmse:0.087123\teval-rmse:0.280332\n",
      "[946]\ttrain-rmse:0.087049\teval-rmse:0.280331\n",
      "[947]\ttrain-rmse:0.086936\teval-rmse:0.280329\n",
      "[948]\ttrain-rmse:0.086876\teval-rmse:0.280324\n",
      "[949]\ttrain-rmse:0.086768\teval-rmse:0.280333\n",
      "[950]\ttrain-rmse:0.086693\teval-rmse:0.280336\n",
      "[951]\ttrain-rmse:0.086548\teval-rmse:0.280338\n",
      "[952]\ttrain-rmse:0.086451\teval-rmse:0.280339\n",
      "[953]\ttrain-rmse:0.086356\teval-rmse:0.280335\n",
      "[954]\ttrain-rmse:0.086241\teval-rmse:0.280321\n",
      "[955]\ttrain-rmse:0.086209\teval-rmse:0.280328\n",
      "[956]\ttrain-rmse:0.086140\teval-rmse:0.280337\n",
      "[957]\ttrain-rmse:0.086094\teval-rmse:0.280333\n",
      "[958]\ttrain-rmse:0.086030\teval-rmse:0.280331\n",
      "[959]\ttrain-rmse:0.085993\teval-rmse:0.280328\n",
      "[960]\ttrain-rmse:0.085910\teval-rmse:0.280319\n",
      "[961]\ttrain-rmse:0.085830\teval-rmse:0.280322\n",
      "[962]\ttrain-rmse:0.085789\teval-rmse:0.280334\n",
      "[963]\ttrain-rmse:0.085729\teval-rmse:0.280348\n",
      "[964]\ttrain-rmse:0.085660\teval-rmse:0.280349\n",
      "[965]\ttrain-rmse:0.085609\teval-rmse:0.280341\n",
      "[966]\ttrain-rmse:0.085539\teval-rmse:0.280342\n",
      "[967]\ttrain-rmse:0.085448\teval-rmse:0.280341\n",
      "[968]\ttrain-rmse:0.085341\teval-rmse:0.280329\n",
      "[969]\ttrain-rmse:0.085231\teval-rmse:0.280337\n",
      "[970]\ttrain-rmse:0.085183\teval-rmse:0.280337\n",
      "[971]\ttrain-rmse:0.085114\teval-rmse:0.280339\n",
      "[972]\ttrain-rmse:0.085026\teval-rmse:0.280328\n",
      "[973]\ttrain-rmse:0.084969\teval-rmse:0.280330\n",
      "[974]\ttrain-rmse:0.084888\teval-rmse:0.280308\n",
      "[975]\ttrain-rmse:0.084810\teval-rmse:0.280312\n",
      "[976]\ttrain-rmse:0.084769\teval-rmse:0.280311\n",
      "[977]\ttrain-rmse:0.084663\teval-rmse:0.280310\n",
      "[978]\ttrain-rmse:0.084587\teval-rmse:0.280319\n",
      "[979]\ttrain-rmse:0.084518\teval-rmse:0.280323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "RMSE: 0.860477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[879]\ttrain-rmse:0.092356\teval-rmse:0.280224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_valid = sts_train[feat3+['Score']].to_dataframe()\n",
    "gbm = train_xgboost(train_valid, feat3, num_boost_round=5000, eta=0.02, max_depth=8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = xgboost_predict(gbm, sts_test[feat2].to_dataframe(), feat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, io\n",
    "\n",
    "def output_to_file(team_name, run_name, target):\n",
    "    test_dir = '../sts2016-english-v1.1/'\n",
    "    filenames = [test_dir+i for i in os.listdir(test_dir) if i.endswith('ascii')]\n",
    "    for infile in filenames:\n",
    "        domain = infile.rpartition('.')[0].rpartition('.')[2]\n",
    "        outfile = 'STS2016.OUTPUT.'+team_name+'.'+run_name+'.'+domain+'.txt'\n",
    "        with io.open(infile, 'r') as fin, io.open(outfile, 'w') as fout:\n",
    "            domain_data = sts_test.filter_by(domain, column_name='Domain')\n",
    "            for line, row in zip(fin, domain_data):\n",
    "                #line = line.split('\\t')\n",
    "                #s1, s2 = line[0], line[1]\n",
    "                fout.write(unicode(row[target])+'\\n')\n",
    "                \n",
    "sts_test.add_column(gl.SArray(m1.predict(sts_test)), name='pred_m1')   \n",
    "sts_test.add_column(gl.SArray(m3.predict(sts_test)), name='pred_m3')\n",
    "sts_test.add_column(gl.SArray(predictions), name='pred_xgboost')\n",
    "output_to_file('saarsheff', 'MT-Metrics-boostedtrees', 'pred_m1')\n",
    "output_to_file('saarsheff', 'MT-Metrics-linear', 'pred_m3')\n",
    "output_to_file('saarsheff', 'MT-Metrics-xgboost', 'pred_xgboost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
